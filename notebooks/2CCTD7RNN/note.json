{
  "paragraphs": [
    {
      "text": "%md\nDatasets use with this tutorial\n---\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:34:52 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDatasets use with this tutorial\u003c/h2\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488309881072_-678258458",
      "id": "20170228-202441_2056011249",
      "dateCreated": "Feb 28, 2017 8:24:41 PM",
      "dateStarted": "Feb 28, 2017 9:05:21 PM",
      "dateFinished": "Feb 28, 2017 9:05:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh \nls -lt /Users/juantomas/proyectos/cursos/curso-spark/datasets\n\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:13:28 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "total 1347920\n-rw-r--r--@ 1 juantomas  staff  689413344 28 feb 22:07 2008.csv\n-rw-r--r--@ 1 juantomas  staff     244438 28 feb 22:07 airports.csv\n-rw-r--r--@ 1 juantomas  staff      43758 28 feb 22:07 carriers.csv\n-rw-r--r--@ 1 juantomas  staff     428796 28 feb 22:07 plane-data.csv\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488311797987_-595394070",
      "id": "20170228-205637_1262468898",
      "dateCreated": "Feb 28, 2017 8:56:37 PM",
      "dateStarted": "Feb 28, 2017 10:13:28 PM",
      "dateFinished": "Feb 28, 2017 10:13:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nUSA Flights 2008 \n\nhttp://stat-computing.org/dataexpo/2009/2008.csv.bz2\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:22:31 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eUSA Flights 2008 \u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\"\u003ehttp://stat-computing.org/dataexpo/2009/2008.csv.bz2\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488316346559_615287004",
      "id": "20170228-221226_181477098",
      "dateCreated": "Feb 28, 2017 10:12:26 PM",
      "dateStarted": "Feb 28, 2017 10:13:18 PM",
      "dateFinished": "Feb 28, 2017 10:13:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nFirst Example (for the impacients)\n---\n\n**Step by Step**: we will start just loading a csv file.",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 11:30:42 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eFirst Example\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStep by Step\u003c/strong\u003e: we will start just loading a csv file.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488316958112_1774440206",
      "id": "20170228-222238_268250743",
      "dateCreated": "Feb 28, 2017 10:22:38 PM",
      "dateStarted": "Feb 28, 2017 10:35:52 PM",
      "dateFinished": "Feb 28, 2017 10:35:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val localPath\u003d\"/Users/juantomas/proyectos/cursos/curso-spark/datasets/\"\nval airports \u003d sqlContext.read.format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(localPath+\"airports.csv\")\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:24:18 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nlocalPath: String \u003d /Users/juantomas/proyectos/cursos/curso-spark/datasets/\n\nairports: org.apache.spark.sql.DataFrame \u003d [iata: string, airport: string ... 5 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488309788667_679951132",
      "id": "20170228-202308_1487435516",
      "dateCreated": "Feb 28, 2017 8:23:08 PM",
      "dateStarted": "Feb 28, 2017 10:24:19 PM",
      "dateFinished": "Feb 28, 2017 10:24:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nWe just create a DataFrame reading the file \nThe schema of the Dataframe is: \n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:28:51 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eWe just create a DataFrame reading the file\u003cbr/\u003eThe schema of the Dataframe is:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488317237175_1793291258",
      "id": "20170228-222717_1840369942",
      "dateCreated": "Feb 28, 2017 10:27:17 PM",
      "dateStarted": "Feb 28, 2017 10:28:51 PM",
      "dateFinished": "Feb 28, 2017 10:28:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nairports.printSchema\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:26:27 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- iata: string (nullable \u003d true)\n |-- airport: string (nullable \u003d true)\n |-- city: string (nullable \u003d true)\n |-- state: string (nullable \u003d true)\n |-- country: string (nullable \u003d true)\n |-- lat: double (nullable \u003d true)\n |-- long: double (nullable \u003d true)\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488317095679_-1440385330",
      "id": "20170228-222455_1852965182",
      "dateCreated": "Feb 28, 2017 10:24:55 PM",
      "dateStarted": "Feb 28, 2017 10:26:28 PM",
      "dateFinished": "Feb 28, 2017 10:26:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nThe contents of the dataframe airports is: \n\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:29:48 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eThe contents of the dataframe airports is:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488317225593_2090286760",
      "id": "20170228-222705_478435638",
      "dateCreated": "Feb 28, 2017 10:27:05 PM",
      "dateStarted": "Feb 28, 2017 10:29:48 PM",
      "dateFinished": "Feb 28, 2017 10:29:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\nairports.show(10)\nprintln(\"Total Airports: \" + airports.count.toString)",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:38:52 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----+--------------------+----------------+-----+-------+-----------+------------+\n|iata|             airport|            city|state|country|        lat|        long|\n+----+--------------------+----------------+-----+-------+-----------+------------+\n| 00M|            Thigpen |     Bay Springs|   MS|    USA|31.95376472|-89.23450472|\n| 00R|Livingston Municipal|      Livingston|   TX|    USA|30.68586111|-95.01792778|\n| 00V|         Meadow Lake|Colorado Springs|   CO|    USA|38.94574889|-104.5698933|\n| 01G|        Perry-Warsaw|           Perry|   NY|    USA|42.74134667|-78.05208056|\n| 01J|    Hilliard Airpark|        Hilliard|   FL|    USA| 30.6880125|-81.90594389|\n| 01M|   Tishomingo County|         Belmont|   MS|    USA|34.49166667|-88.20111111|\n| 02A|         Gragg-Wade |         Clanton|   AL|    USA|32.85048667|-86.61145333|\n| 02C|             Capitol|      Brookfield|   WI|    USA|   43.08751|-88.17786917|\n| 02G|   Columbiana County|  East Liverpool|   OH|    USA|40.67331278|-80.64140639|\n| 03D|    Memphis Memorial|         Memphis|   MO|    USA|40.44725889|-92.22696056|\n+----+--------------------+----------------+-----+-------+-----------+------------+\nonly showing top 10 rows\n\nTotal Airports: 3376\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488316843497_847394571",
      "id": "20170228-222043_2046966645",
      "dateCreated": "Feb 28, 2017 10:20:43 PM",
      "dateStarted": "Feb 28, 2017 10:38:52 PM",
      "dateFinished": "Feb 28, 2017 10:38:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nval flights \u003d sqlContext.read.format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(localPath+\"2008.csv\")\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:40:53 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nflights: org.apache.spark.sql.DataFrame \u003d [Year: int, Month: int ... 27 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488317167429_-1582434295",
      "id": "20170228-222607_1793357259",
      "dateCreated": "Feb 28, 2017 10:26:07 PM",
      "dateStarted": "Feb 28, 2017 10:40:53 PM",
      "dateFinished": "Feb 28, 2017 10:41:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nflights.printSchema\nflights.show(10)\nprintln(\"Toatal: \" + flights.count)\n\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:43:33 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 304.0,
              "optionOpen": false
            }
          }
        },
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- Year: integer (nullable \u003d true)\n |-- Month: integer (nullable \u003d true)\n |-- DayofMonth: integer (nullable \u003d true)\n |-- DayOfWeek: integer (nullable \u003d true)\n |-- DepTime: string (nullable \u003d true)\n |-- CRSDepTime: integer (nullable \u003d true)\n |-- ArrTime: string (nullable \u003d true)\n |-- CRSArrTime: integer (nullable \u003d true)\n |-- UniqueCarrier: string (nullable \u003d true)\n |-- FlightNum: integer (nullable \u003d true)\n |-- TailNum: string (nullable \u003d true)\n |-- ActualElapsedTime: string (nullable \u003d true)\n |-- CRSElapsedTime: string (nullable \u003d true)\n |-- AirTime: string (nullable \u003d true)\n |-- ArrDelay: string (nullable \u003d true)\n |-- DepDelay: string (nullable \u003d true)\n |-- Origin: string (nullable \u003d true)\n |-- Dest: string (nullable \u003d true)\n |-- Distance: integer (nullable \u003d true)\n |-- TaxiIn: string (nullable \u003d true)\n |-- TaxiOut: string (nullable \u003d true)\n |-- Cancelled: integer (nullable \u003d true)\n |-- CancellationCode: string (nullable \u003d true)\n |-- Diverted: integer (nullable \u003d true)\n |-- CarrierDelay: string (nullable \u003d true)\n |-- WeatherDelay: string (nullable \u003d true)\n |-- NASDelay: string (nullable \u003d true)\n |-- SecurityDelay: string (nullable \u003d true)\n |-- LateAircraftDelay: string (nullable \u003d true)\n\n+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n|2008|    1|         3|        4|   2003|      1955|   2211|      2225|           WN|      335| N712SW|              128|           150|    116|     -14|       8|   IAD| TPA|     810|     4|      8|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|    754|       735|   1002|      1000|           WN|     3231| N772SW|              128|           145|    113|       2|      19|   IAD| TPA|     810|     5|     10|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|    628|       620|    804|       750|           WN|      448| N428WN|               96|            90|     76|      14|       8|   IND| BWI|     515|     3|     17|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|    926|       930|   1054|      1100|           WN|     1746| N612SW|               88|            90|     78|      -6|      -4|   IND| BWI|     515|     3|      7|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|   1829|      1755|   1959|      1925|           WN|     3920| N464WN|               90|            90|     77|      34|      34|   IND| BWI|     515|     3|     10|        0|            null|       0|           2|           0|       0|            0|               32|\n|2008|    1|         3|        4|   1940|      1915|   2121|      2110|           WN|      378| N726SW|              101|           115|     87|      11|      25|   IND| JAX|     688|     4|     10|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|   1937|      1830|   2037|      1940|           WN|      509| N763SW|              240|           250|    230|      57|      67|   IND| LAS|    1591|     3|      7|        0|            null|       0|          10|           0|       0|            0|               47|\n|2008|    1|         3|        4|   1039|      1040|   1132|      1150|           WN|      535| N428WN|              233|           250|    219|     -18|      -1|   IND| LAS|    1591|     7|      7|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|    617|       615|    652|       650|           WN|       11| N689SW|               95|            95|     70|       2|       2|   IND| MCI|     451|     6|     19|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|   1620|      1620|   1639|      1655|           WN|      810| N648SW|               79|            95|     70|     -16|       0|   IND| MCI|     451|     3|      6|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\nonly showing top 10 rows\n\nToatal: 7009728\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488318053270_1234204013",
      "id": "20170228-224053_1830699528",
      "dateCreated": "Feb 28, 2017 10:40:53 PM",
      "dateStarted": "Feb 28, 2017 10:42:36 PM",
      "dateFinished": "Feb 28, 2017 10:42:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n\nSo what is Spark SQL? \n---\n\n\n* At the very start was a derivate of HIVEQL \n* Spark SQL is a Spark library that runs on top of Spark\n* It provides a higher-level abstraction than the Spark core API for processing structured data\n* Structured data includes data stored in a database, NoSQL data store, Parquet, ORC, Avro, JSON, CSV, or any other structured format\n* The NoSQL data stores that can be used with Spark SQL include HBase, Cassandra, Elasticsearch, Druid, and other NoSQL data stores\n* Spark SQL is more than just about providing SQL interface to Spark\n* Spark SQL can be used as a library for developing data processing applications in Scala, Java, Python, or R\n* internally uses the Spark core API to execute queries on a Spark cluster\n* Spark SQL seamlessly integrates with other Spark libraries such as Spark Streaming, Spark ML, and GraphX\n\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 11:37:54 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSo what is Spark SQL?\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eAt the very start was a derivate of HIVEQL\u003c/li\u003e\n  \u003cli\u003eSpark SQL is a Spark library that runs on top of Spark\u003c/li\u003e\n  \u003cli\u003eIt provides a higher-level abstraction than the Spark core API for processing structured data\u003c/li\u003e\n  \u003cli\u003eStructured data includes data stored in a database, NoSQL data store, Parquet, ORC, Avro, JSON, CSV, or any other structured format\u003c/li\u003e\n  \u003cli\u003eThe NoSQL data stores that can be used with Spark SQL include HBase, Cassandra, Elasticsearch, Druid, and other NoSQL data stores\u003c/li\u003e\n  \u003cli\u003eSpark SQL is more than just about providing SQL interface to Spark\u003c/li\u003e\n  \u003cli\u003eSpark SQL can be used as a library for developing data processing applications in Scala, Java, Python, or R\u003c/li\u003e\n  \u003cli\u003einternally uses the Spark core API to execute queries on a Spark cluster\u003c/li\u003e\n  \u003cli\u003eSpark SQL seamlessly integrates with other Spark libraries such as Spark Streaming, Spark ML, and GraphX\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488318155881_277638337",
      "id": "20170228-224235_216639834",
      "dateCreated": "Feb 28, 2017 10:42:35 PM",
      "dateStarted": "Feb 28, 2017 11:37:54 PM",
      "dateFinished": "Feb 28, 2017 11:37:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nThat\u0027s the global view of spark \u0026 spark SQL \n---\n\n![alternate text](https://www.safaribooksonline.com/library/view/big-data-analytics/9781484209646/images/9781484209653_Fig07-01.jpg)\n\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 11:55:40 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eThat\u0026rsquo;s the global view of spark \u0026amp; spark SQL\u003c/h2\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://www.safaribooksonline.com/library/view/big-data-analytics/9781484209646/images/9781484209653_Fig07-01.jpg\" alt\u003d\"alternate text\" /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488318804163_-1162528226",
      "id": "20170228-225324_353156368",
      "dateCreated": "Feb 28, 2017 10:53:24 PM",
      "dateStarted": "Feb 28, 2017 11:55:40 PM",
      "dateFinished": "Feb 28, 2017 11:55:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n\nKeypoints of Spark SQL \n---\n\n**Performance**\n\n\n*TL;DR*: Spark SQL makes data processing applications run faster using a combination of techniques, including reduced disk I/O, in-memory columnar caching, query optimization, and code generation.\n\n\n* Reduced Disk I/O\n    * Disk I/O is slow\n    * can skip non-required partitions, rows, or columns while reading data.\n\n* Partitioning\n    * a lot of I/O can be avoided by partitioning a dataset\n    \n* Columnar Storage\n    * Spark SQL supports columnar storage formats such as Parquet, which allow reading of only the columns that are used in a query\n\n* In-Memory Columnar Caching\n    * Spark SQL allows an application to cache data in an in-memory columnar format from any data source\n    * Spark SQL, caches only the required columns\n    * Spark SQL compresses the cached columns to minimize memory usage and JVM garbage collection pressure\n    * apply efficient compression techniques such as run length encoding, delta encoding, and dictionary encoding\n    \n* Skip Rows\n    * If a data source maintains statistical information about a dataset, Spark SQL takes advantage of it\n    * Serialization formats such as Parquet and ORC store min and max values for each column in a row group or chunk of rows\n\n* Predicate Pushdown\n    * Instead of reading an entire table and then executing a filtering operation, Spark SQL will ask the database to natively execute the filtering operation\n    * Since databases generally index data, native filtering is much faster than filtering at the application layer\n    \n* Query Optimization\n    * analysis\n    * logical optimization\n    * physical planning\n    * code generation\n\n* Applications \n    * Spark SQL can be used as a ETL\n    * Very fast conversions between formats.\n    * Few lines of code.\n    * You can even perform join operations across different data sources.\n\n* Distributed JDBC/ODBC SQL Query Engine\n    * can be used as a library. In this mode, data processing tasks can be expressed as SQL, HiveQL or language integrated queries within a Scala, Java, Python, or R application\n    * It comes prepackaged with a Thrift/JDBC/ODBC server\n    \n* Data Warehousing\n    * Spark SQL store and analyze large amounts of data like a warehouse\n    * Spark SQL-based data warehousing solution is more scalable, economical, and flexible than the proprietary data warehousing solutions\n    * is flexible since it supports both schema-on-read and schema-on-write",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:18:31 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eKeypoints of Spark SQL\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ePerformance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eTL;DR\u003c/em\u003e: Spark SQL makes data processing applications run faster using a combination of techniques, including reduced disk I/O, in-memory columnar caching, query optimization, and code generation.\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eReduced Disk I/O\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eDisk I/O is slow\u003c/li\u003e\n      \u003cli\u003ecan skip non-required partitions, rows, or columns while reading data.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003ePartitioning\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003ea lot of I/O can be avoided by partitioning a dataset\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eColumnar Storage\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSpark SQL supports columnar storage formats such as Parquet, which allow reading of only the columns that are used in a query\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eIn-Memory Columnar Caching\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSpark SQL allows an application to cache data in an in-memory columnar format from any data source\u003c/li\u003e\n      \u003cli\u003eSpark SQL, caches only the required columns\u003c/li\u003e\n      \u003cli\u003eSpark SQL compresses the cached columns to minimize memory usage and JVM garbage collection pressure\u003c/li\u003e\n      \u003cli\u003eapply efficient compression techniques such as run length encoding, delta encoding, and dictionary encoding\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eSkip Rows\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eIf a data source maintains statistical information about a dataset, Spark SQL takes advantage of it\u003c/li\u003e\n      \u003cli\u003eSerialization formats such as Parquet and ORC store min and max values for each column in a row group or chunk of rows\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003ePredicate Pushdown\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eInstead of reading an entire table and then executing a filtering operation, Spark SQL will ask the database to natively execute the filtering operation\u003c/li\u003e\n      \u003cli\u003eSince databases generally index data, native filtering is much faster than filtering at the application layer\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eQuery Optimization\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eanalysis\u003c/li\u003e\n      \u003cli\u003elogical optimization\u003c/li\u003e\n      \u003cli\u003ephysical planning\u003c/li\u003e\n      \u003cli\u003ecode generation\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eApplications \u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSpark SQL can be used as a ETL\u003c/li\u003e\n      \u003cli\u003eVery fast conversions between formats.\u003c/li\u003e\n      \u003cli\u003eFew lines of code.\u003c/li\u003e\n      \u003cli\u003eYou can even perform join operations across different data sources.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eDistributed JDBC/ODBC SQL Query Engine\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003ecan be used as a library. In this mode, data processing tasks can be expressed as SQL, HiveQL or language integrated queries within a Scala, Java, Python, or R application\u003c/li\u003e\n      \u003cli\u003eIt comes prepackaged with a Thrift/JDBC/ODBC server\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eData Warehousing\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSpark SQL store and analyze large amounts of data like a warehouse\u003c/li\u003e\n      \u003cli\u003eSpark SQL-based data warehousing solution is more scalable, economical, and flexible than the proprietary data warehousing solutions\u003c/li\u003e\n      \u003cli\u003eis flexible since it supports both schema-on-read and schema-on-write\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488321602101_-1180240952",
      "id": "20170228-234002_1917995362",
      "dateCreated": "Feb 28, 2017 11:40:02 PM",
      "dateStarted": "Mar 1, 2017 12:18:31 AM",
      "dateFinished": "Mar 1, 2017 12:18:31 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nApplication Programming Interface (API)\n---\n\nThe Spark SQL API consists of three key abstractions: \n\n* SQLContext\n* HiveContext \n* DataFrame. \n\nA Spark SQL application processes data using these abstractions.\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:20:32 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eApplication Programming Interface (API)\u003c/h2\u003e\n\u003cp\u003eThe Spark SQL API consists of three key abstractions: \u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eSQLContext\u003c/li\u003e\n  \u003cli\u003eHiveContext\u003c/li\u003e\n  \u003cli\u003eDataFrame.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA Spark SQL application processes data using these abstractions.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488321850550_-605473712",
      "id": "20170228-234410_1366267029",
      "dateCreated": "Feb 28, 2017 11:44:10 PM",
      "dateStarted": "Mar 1, 2017 12:20:32 AM",
      "dateFinished": "Mar 1, 2017 12:20:33 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nSQLContext \n---\n\nSQLContext is the main entry point into the Spark SQL library.\n\nIt is a class defined in the Spark SQL library.\n\nA Spark SQL application must create an instance of the SQLContext or HiveContext class.\n\n**How create it**\n\n~~~\nimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\"My Spark SQL app\")\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\n\nval resultSet \u003d sqlContext.sql(\"SELECT count(1) FROM my_table\")\n~~~",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:30:32 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "tableHide": false,
        "editorHide": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSQLContext\u003c/h2\u003e\n\u003cp\u003eSQLContext is the main entry point into the Spark SQL library.\u003c/p\u003e\n\u003cp\u003eIt is a class defined in the Spark SQL library.\u003c/p\u003e\n\u003cp\u003eA Spark SQL application must create an instance of the SQLContext or HiveContext class.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHow create it\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\u0026quot;My Spark SQL app\u0026quot;)\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\n\nval resultSet \u003d sqlContext.sql(\u0026quot;SELECT count(1) FROM my_table\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488324031777_1893317446",
      "id": "20170301-002031_1597575652",
      "dateCreated": "Mar 1, 2017 12:20:31 AM",
      "dateStarted": "Mar 1, 2017 12:30:26 AM",
      "dateFinished": "Mar 1, 2017 12:30:26 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark \n\nsc \nsqlContext\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:25:02 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nres11: org.apache.spark.SparkContext \u003d org.apache.spark.SparkContext@3a63e394\n\nres12: org.apache.spark.sql.SQLContext \u003d org.apache.spark.sql.SQLContext@472983b3\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488324247517_-1005305790",
      "id": "20170301-002407_1426558498",
      "dateCreated": "Mar 1, 2017 12:24:07 AM",
      "dateStarted": "Mar 1, 2017 12:25:03 AM",
      "dateFinished": "Mar 1, 2017 12:25:09 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nDataFrames\n---\n\n* DataFrame is Spark SQL’s primary data abstraction\n* a distributed collection of rows organized into named columns.\n* inspired by DataFrames in R and Python\n* similar to a table in a relational database\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:33:13 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDataFrames\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eDataFrame is Spark SQL’s primary data abstraction\u003c/li\u003e\n  \u003cli\u003ea distributed collection of rows organized into named columns.\u003c/li\u003e\n  \u003cli\u003einspired by DataFrames in R and Python\u003c/li\u003e\n  \u003cli\u003esimilar to a table in a relational database\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488324302565_688651672",
      "id": "20170301-002502_1565229103",
      "dateCreated": "Mar 1, 2017 12:25:02 AM",
      "dateStarted": "Mar 1, 2017 12:33:13 AM",
      "dateFinished": "Mar 1, 2017 12:33:13 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nDiffs between RDDs \u0026 Dataframes\n---\n\n* DataFrame is schema aware\n* RDD is a partitioned collection of opaque elements\n* DataFrame knows the names and types of the columns in a dataset\n* DataFrame class is able to provide a rich domain-specific-language (DSL) for data processing\n* DataFrame API is easier to understand and use than the RDD API\n* DataFrame can also be operated on as an RDD\n* RDD can be easily created from a DataFrame\n* DataFrame can be registered as a temporary table\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:37:22 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDiffs between RDDs \u0026amp; Dataframes\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eDataFrame is schema aware\u003c/li\u003e\n  \u003cli\u003eRDD is a partitioned collection of opaque elements\u003c/li\u003e\n  \u003cli\u003eDataFrame knows the names and types of the columns in a dataset\u003c/li\u003e\n  \u003cli\u003eDataFrame class is able to provide a rich domain-specific-language (DSL) for data processing\u003c/li\u003e\n  \u003cli\u003eDataFrame API is easier to understand and use than the RDD API\u003c/li\u003e\n  \u003cli\u003eDataFrame can also be operated on as an RDD\u003c/li\u003e\n  \u003cli\u003eRDD can be easily created from a DataFrame\u003c/li\u003e\n  \u003cli\u003eDataFrame can be registered as a temporary table\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488324777727_-1194054392",
      "id": "20170301-003257_151015170",
      "dateCreated": "Mar 1, 2017 12:32:57 AM",
      "dateStarted": "Mar 1, 2017 12:37:23 AM",
      "dateFinished": "Mar 1, 2017 12:37:23 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nRow\n---\n\nRow is a Spark SQL abstraction for representing a row of data\n\nit is equivalent to a relational tuple or row in a table\n\nYou can create a Row this way: \n\n~~~\nimport org.apache.spark.sql._\n\nval row1 \u003d Row(\"Barack Obama\", \"President\", \"United States\")\nval row2 \u003d Row(\"David Cameron\", \"Prime Minister\", \"United Kingdom\")\n~~~\n\nand you can use this rows at low level: \n\n~~~\nval presidentName \u003d row1.getString(0)\nval country \u003d row1.getString(2)\n~~~",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:40:07 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eRow\u003c/h2\u003e\n\u003cp\u003eRow is a Spark SQL abstraction for representing a row of data\u003c/p\u003e\n\u003cp\u003eit is equivalent to a relational tuple or row in a table\u003c/p\u003e\n\u003cp\u003eYou can create a Row this way: \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.sql._\n\nval row1 \u003d Row(\u0026quot;Barack Obama\u0026quot;, \u0026quot;President\u0026quot;, \u0026quot;United States\u0026quot;)\nval row2 \u003d Row(\u0026quot;David Cameron\u0026quot;, \u0026quot;Prime Minister\u0026quot;, \u0026quot;United Kingdom\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand you can use this rows at low level: \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval presidentName \u003d row1.getString(0)\nval country \u003d row1.getString(2)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488325042529_1230631532",
      "id": "20170301-003722_1470503227",
      "dateCreated": "Mar 1, 2017 12:37:22 AM",
      "dateStarted": "Mar 1, 2017 12:40:07 AM",
      "dateFinished": "Mar 1, 2017 12:40:07 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nHow to create a Dataframe: \n---\n\n**a) from an RDD** \n\n~~~\nimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\"My Spark SQL app\")\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\nimport sqlContext.implicits._\n\ncase class Employee(name: String, age: Int, gender: String)\n\nval rowsRDD \u003d sc.textFile(\"path/to/employees.csv\")\nval employeesRDD \u003d rowsRDD.map{row \u003d\u003e row.split(\",\")}\n                          .map{cols \u003d\u003e Employee(cols(0), cols(1).trim.toInt, cols(2))}\n\nval employeesDF \u003d employeesRDD.toDF()\n\n~~~\n\n**b) createDataFrame**\n\n~~~\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\n\nval config \u003d new SparkConf().setAppName(\"My Spark SQL app\")\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\n\nval linesRDD \u003d sc.textFile(\"path/to/employees.csv\")\nval rowsRDD \u003d linesRDD.map{row \u003d\u003e row.split(\",\")}\n                      .map{cols \u003d\u003e Row(cols(0), cols(1).trim.toInt, cols(2))}\n\nval schema \u003d StructType(List(\n                 StructField(\"name\", StringType, false),\n                 StructField(\"age\", IntegerType, false),\n                 StructField(\"gender\", StringType, false)\n               )\n             )\n\nval employeesDF \u003d sqlContext.createDataFrame(rowsRDD,schema)\n~~~\n\n**c) Creating a DataFrame from a Data Source**\n\nSpark SQL provides a unified interface for creating a DataFrame from a variety of data sources.\n\nAPI can be used to create a DataFrame from a MySQL, PostgreSQL, Oracle, or Cassandra table.\n\nthe same API can be used to create a DataFrame from a Parquet, JSON, ORC or CSV file on local file system, HDFS or S3\n\nSpark SQL has built-in support for some of the commonly used data sources\n\nIncludes Parquet, JSON, Hive, and JDBC-compliant databases\n\nExternal packages are available for other data sources\n\n~~~\nimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\"My Spark SQL app\")\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new org.apache.spark.sql.hive.HiveContext (sc)\n\n// create a DataFrame from parquet files\nval parquetDF \u003d sqlContext.read\n                          .format(\"org.apache.spark.sql.parquet\")\n                          .load(\"path/to/Parquet-file-or-directory\")\n\n// create a DataFrame from JSON files\nval jsonDF \u003d sqlContext.read\n                       .format(\"org.apache.spark.sql.json\")\n                       .load(\"path/to/JSON-file-or-directory\")\n\n// create a DataFrame from a table in a Postgres database\nval jdbcDF \u003d sqlContext.read\n               .format(\"org.apache.spark.sql.jdbc\")\n               .options(Map(\n                  \"url\" -\u003e \"jdbc:postgresql://host:port/database?user\u003d\u003cUSER\u003e\u0026password\u003d\u003cPASS\u003e\",\n                  \"dbtable\" -\u003e \"schema-name.table-name\"))\n               .load()\n\n// create a DataFrame from a Hive table\nval hiveDF \u003d sqlContext.read\n                       .table(\"hive-table-name\")\n~~~\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:50:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eHow to create a Dataframe:\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ea) from an RDD\u003c/strong\u003e \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\u0026quot;My Spark SQL app\u0026quot;)\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\nimport sqlContext.implicits._\n\ncase class Employee(name: String, age: Int, gender: String)\n\nval rowsRDD \u003d sc.textFile(\u0026quot;path/to/employees.csv\u0026quot;)\nval employeesRDD \u003d rowsRDD.map{row \u003d\u0026gt; row.split(\u0026quot;,\u0026quot;)}\n                          .map{cols \u003d\u0026gt; Employee(cols(0), cols(1).trim.toInt, cols(2))}\n\nval employeesDF \u003d employeesRDD.toDF()\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eb) createDataFrame\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\n\nval config \u003d new SparkConf().setAppName(\u0026quot;My Spark SQL app\u0026quot;)\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\n\nval linesRDD \u003d sc.textFile(\u0026quot;path/to/employees.csv\u0026quot;)\nval rowsRDD \u003d linesRDD.map{row \u003d\u0026gt; row.split(\u0026quot;,\u0026quot;)}\n                      .map{cols \u003d\u0026gt; Row(cols(0), cols(1).trim.toInt, cols(2))}\n\nval schema \u003d StructType(List(\n                 StructField(\u0026quot;name\u0026quot;, StringType, false),\n                 StructField(\u0026quot;age\u0026quot;, IntegerType, false),\n                 StructField(\u0026quot;gender\u0026quot;, StringType, false)\n               )\n             )\n\nval employeesDF \u003d sqlContext.createDataFrame(rowsRDD,schema)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ec) Creating a DataFrame from a Data Source\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpark SQL provides a unified interface for creating a DataFrame from a variety of data sources.\u003c/p\u003e\n\u003cp\u003eAPI can be used to create a DataFrame from a MySQL, PostgreSQL, Oracle, or Cassandra table.\u003c/p\u003e\n\u003cp\u003ethe same API can be used to create a DataFrame from a Parquet, JSON, ORC or CSV file on local file system, HDFS or S3\u003c/p\u003e\n\u003cp\u003eSpark SQL has built-in support for some of the commonly used data sources\u003c/p\u003e\n\u003cp\u003eIncludes Parquet, JSON, Hive, and JDBC-compliant databases\u003c/p\u003e\n\u003cp\u003eExternal packages are available for other data sources\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\u0026quot;My Spark SQL app\u0026quot;)\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new org.apache.spark.sql.hive.HiveContext (sc)\n\n// create a DataFrame from parquet files\nval parquetDF \u003d sqlContext.read\n                          .format(\u0026quot;org.apache.spark.sql.parquet\u0026quot;)\n                          .load(\u0026quot;path/to/Parquet-file-or-directory\u0026quot;)\n\n// create a DataFrame from JSON files\nval jsonDF \u003d sqlContext.read\n                       .format(\u0026quot;org.apache.spark.sql.json\u0026quot;)\n                       .load(\u0026quot;path/to/JSON-file-or-directory\u0026quot;)\n\n// create a DataFrame from a table in a Postgres database\nval jdbcDF \u003d sqlContext.read\n               .format(\u0026quot;org.apache.spark.sql.jdbc\u0026quot;)\n               .options(Map(\n                  \u0026quot;url\u0026quot; -\u0026gt; \u0026quot;jdbc:postgresql://host:port/database?user\u003d\u0026lt;USER\u0026gt;\u0026amp;password\u003d\u0026lt;PASS\u0026gt;\u0026quot;,\n                  \u0026quot;dbtable\u0026quot; -\u0026gt; \u0026quot;schema-name.table-name\u0026quot;))\n               .load()\n\n// create a DataFrame from a Hive table\nval hiveDF \u003d sqlContext.read\n                       .table(\u0026quot;hive-table-name\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488325207232_-997820417",
      "id": "20170301-004007_185660401",
      "dateCreated": "Mar 1, 2017 12:40:07 AM",
      "dateStarted": "Mar 1, 2017 12:50:52 AM",
      "dateFinished": "Mar 1, 2017 12:50:52 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nSome examples reading from different sources \n---\n\n**JSON**\n\n~~~\nval jsonDF \u003d sqlContext.read.json(\"path/to/JSON-file-or-directory\")\nval jsonHdfsDF \u003d sqlContext.read.json(\"hdfs://NAME_NODE/path/to/data.json\")\nval jsonS3DF \u003d sqlContext.read.json(\"s3a://BUCKET_NAME/FOLDER_NAME/data.json\")\n~~~\nPassing the schema to dataframe reader: \n~~~\nimport org.apache.spark.sql.types._\n\nval userSchema \u003d StructType(List(\n                     StructField(\"name\", StringType, false),\n                     StructField(\"age\", IntegerType, false),\n                     StructField(\"gender\", StringType, false)\n                     )\n                   )\n\nval userDF \u003d sqlContext.read\n                       .schema(userSchema)\n                       .json(\"path/to/user.json\")\n~~~\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:54:41 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSome examples reading from different sources\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eJSON\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval jsonDF \u003d sqlContext.read.json(\u0026quot;path/to/JSON-file-or-directory\u0026quot;)\nval jsonHdfsDF \u003d sqlContext.read.json(\u0026quot;hdfs://NAME_NODE/path/to/data.json\u0026quot;)\nval jsonS3DF \u003d sqlContext.read.json(\u0026quot;s3a://BUCKET_NAME/FOLDER_NAME/data.json\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePassing the schema to dataframe reader: \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.sql.types._\n\nval userSchema \u003d StructType(List(\n                     StructField(\u0026quot;name\u0026quot;, StringType, false),\n                     StructField(\u0026quot;age\u0026quot;, IntegerType, false),\n                     StructField(\u0026quot;gender\u0026quot;, StringType, false)\n                     )\n                   )\n\nval userDF \u003d sqlContext.read\n                       .schema(userSchema)\n                       .json(\u0026quot;path/to/user.json\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488325610148_-2044538284",
      "id": "20170301-004650_654668507",
      "dateCreated": "Mar 1, 2017 12:46:50 AM",
      "dateStarted": "Mar 1, 2017 12:54:43 AM",
      "dateFinished": "Mar 1, 2017 12:54:43 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n**Parquet**\n~~~\nval parquetDF \u003d sqlContext.read.parquet(\"path/to/parquet-file-or-directory\")\n~~~\n\n**ORC**\n\n~~~\nval orcDF \u003d hiveContext.read.orc(\"path/to/orc-file-or-directory\")\n~~~\n\n**Hive**\n\n~~~\nval hiveDF \u003d hiveContext.read.table(\"hive-table-name\")\nval hiveDF \u003d hiveContext.sql(\"SELECT col_a, col_b, col_c from hive-table\")\n~~~\n\n**JDBC**\n\n~~~\nval jdbcUrl \u003d\"jdbc:mysql://host:port/database\"\nval tableName \u003d \"table-name\"\nval connectionProperties \u003d new java.util.Properties\nconnectionProperties.setProperty(\"user\",\"database-user-name\")\nconnectionProperties.setProperty(\"password\",\" database-user-password\")\n\nval jdbcDF \u003d hiveContext.read\n                        .jdbc(jdbcUrl, tableName, connectionProperties)\n~~~\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:59:48 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e\u003cstrong\u003eParquet\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval parquetDF \u003d sqlContext.read.parquet(\u0026quot;path/to/parquet-file-or-directory\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eORC\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval orcDF \u003d hiveContext.read.orc(\u0026quot;path/to/orc-file-or-directory\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eHive\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval hiveDF \u003d hiveContext.read.table(\u0026quot;hive-table-name\u0026quot;)\nval hiveDF \u003d hiveContext.sql(\u0026quot;SELECT col_a, col_b, col_c from hive-table\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eJDBC\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval jdbcUrl \u003d\u0026quot;jdbc:mysql://host:port/database\u0026quot;\nval tableName \u003d \u0026quot;table-name\u0026quot;\nval connectionProperties \u003d new java.util.Properties\nconnectionProperties.setProperty(\u0026quot;user\u0026quot;,\u0026quot;database-user-name\u0026quot;)\nconnectionProperties.setProperty(\u0026quot;password\u0026quot;,\u0026quot; database-user-password\u0026quot;)\n\nval jdbcDF \u003d hiveContext.read\n                        .jdbc(jdbcUrl, tableName, connectionProperties)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488326081842_1797425193",
      "id": "20170301-005441_1241995655",
      "dateCreated": "Mar 1, 2017 12:54:41 AM",
      "dateStarted": "Mar 1, 2017 12:59:41 AM",
      "dateFinished": "Mar 1, 2017 12:59:41 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nBasic Operations\n---\n\n**cache**\n\nThe cache method stores the source DataFrame in memory using a columnar format. It scans only the required columns and stores them in compressed in-memory columnar format. Spark SQL automatically selects a compression codec for each column based on data statistics.\n\n~~~\ncustomerDF.cache()\n~~~\n\nThe caching functionality can be tuned using the setConf method in the SQLContext or HiveContext class. The two configuration parameters for caching are spark.sql.inMemoryColumnarStorage.compressed and spark.sql.inMemoryColumnarStorage.batchSize. By default, compression is turned on and the batch size for columnar caching is 10,000.\n~~~\nsqlContext.setConf(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\")\nsqlContext.setConf(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"10000\")\n~~~\n\n**columns**\n\nThe columns method returns the names of all the columns in the source DataFrame as an array of String.\n~~~\nval cols \u003d customerDF.columns\n\ncols: Array[String] \u003d Array(cId, name, age, gender)\n~~~\n\n**dtypes**\n\nThe dtypes method returns the data types of all the columns in the source DataFrame as an array of tuples. The first element in a tuple is the name of a column and the second element is the data type of that column.\n\n~~~\nval columnsWithTypes \u003d customerDF.dtypes\n\ncolumnsWithTypes: Array[(String, String)] \u003d Array((cId,LongType), (name,StringType), (age,IntegerType), (gender,StringType))\n~~~\n\n**explain**\n\nThe explain method prints the physical plan on the console. It is useful for debugging.\n~~~\ncustomerDF.explain()\n\n\u003d\u003d Physical Plan \u003d\u003d\nInMemoryColumnarTableScan [cId#0L,name#1,age#2,gender#3], (InMemoryRelation [cId#0L,name#1,age#2,gender#3], true, 10000, StorageLevel(true, true, false, true, 1), (Scan PhysicalRDD[cId#0L,name#1,age#2,gender#3]), None)\n~~~\n\nA variant of the explain method takes a Boolean argument and prints both logical and physical plans if the argument is true.\n\n**persist**\n\nThe persist method caches the source DataFrame in memory.\n\n~~~\ncustomerDF.persist\n~~~\nSimilar to the persist method in the RDD class, a variant of the persist method in the DataFrame class allows you to specify the storage level for a persisted DataFrame.\n\n**printSchema**\n\nThe printSchema method prints the schema of the source DataFrame on the console in a tree format.\n~~~\ncustomerDF.printSchema()\n\nroot\n |-- cId: long (nullable \u003d false)\n |-- name: string (nullable \u003d true)\n |-- age: integer (nullable \u003d false)\n |-- gender: string (nullable \u003d true)\n~~~\n\n**registerTempTable**\n\nThe registerTempTable method creates a temporary table in Hive metastore. It takes a table name as an argument.\n\nA temporary table can be queried using the sql method in SQLContext or HiveContext. It is available only during the lifespan of the application that creates it.\n~~~\ncustomerDF.registerTempTable(\"customer\")\nval countDF \u003d sqlContext.sql(\"SELECT count(1) AS cnt FROM customer\")\n\ncountDF: org.apache.spark.sql.DataFrame \u003d [cnt: bigint]\n~~~\nNote that the sql method returns a DataFrame. Let’s discuss how to view or retrieve the contents of a DataFrame in a moment.\n\n**toDF**\n\nThe toDF method allows you to rename the columns in the source DataFrame. It takes the new names of the columns as arguments and returns a new DataFrame.\n~~~\nval resultDF \u003d sqlContext.sql(\"SELECT count(1) from customer\")\n\nresultDF: org.apache.spark.sql.DataFrame \u003d [_c0: bigint]\n\nval countDF \u003d resultDF.toDF(\"cnt\")\n\ncountDF: org.apache.spark.sql.DataFrame \u003d [cnt: bigint]\n\n~~~\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 1:25:11 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eBasic Operations\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ecache\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe cache method stores the source DataFrame in memory using a columnar format. It scans only the required columns and stores them in compressed in-memory columnar format. Spark SQL automatically selects a compression codec for each column based on data statistics.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.cache()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe caching functionality can be tuned using the setConf method in the SQLContext or HiveContext class. The two configuration parameters for caching are spark.sql.inMemoryColumnarStorage.compressed and spark.sql.inMemoryColumnarStorage.batchSize. By default, compression is turned on and the batch size for columnar caching is 10,000.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.setConf(\u0026quot;spark.sql.inMemoryColumnarStorage.compressed\u0026quot;, \u0026quot;true\u0026quot;)\nsqlContext.setConf(\u0026quot;spark.sql.inMemoryColumnarStorage.batchSize\u0026quot;, \u0026quot;10000\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ecolumns\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe columns method returns the names of all the columns in the source DataFrame as an array of String.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval cols \u003d customerDF.columns\n\ncols: Array[String] \u003d Array(cId, name, age, gender)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003edtypes\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe dtypes method returns the data types of all the columns in the source DataFrame as an array of tuples. The first element in a tuple is the name of a column and the second element is the data type of that column.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval columnsWithTypes \u003d customerDF.dtypes\n\ncolumnsWithTypes: Array[(String, String)] \u003d Array((cId,LongType), (name,StringType), (age,IntegerType), (gender,StringType))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eexplain\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe explain method prints the physical plan on the console. It is useful for debugging.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.explain()\n\n\u003d\u003d Physical Plan \u003d\u003d\nInMemoryColumnarTableScan [cId#0L,name#1,age#2,gender#3], (InMemoryRelation [cId#0L,name#1,age#2,gender#3], true, 10000, StorageLevel(true, true, false, true, 1), (Scan PhysicalRDD[cId#0L,name#1,age#2,gender#3]), None)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA variant of the explain method takes a Boolean argument and prints both logical and physical plans if the argument is true.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003epersist\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe persist method caches the source DataFrame in memory.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.persist\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSimilar to the persist method in the RDD class, a variant of the persist method in the DataFrame class allows you to specify the storage level for a persisted DataFrame.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eprintSchema\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe printSchema method prints the schema of the source DataFrame on the console in a tree format.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.printSchema()\n\nroot\n |-- cId: long (nullable \u003d false)\n |-- name: string (nullable \u003d true)\n |-- age: integer (nullable \u003d false)\n |-- gender: string (nullable \u003d true)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eregisterTempTable\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe registerTempTable method creates a temporary table in Hive metastore. It takes a table name as an argument.\u003c/p\u003e\n\u003cp\u003eA temporary table can be queried using the sql method in SQLContext or HiveContext. It is available only during the lifespan of the application that creates it.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.registerTempTable(\u0026quot;customer\u0026quot;)\nval countDF \u003d sqlContext.sql(\u0026quot;SELECT count(1) AS cnt FROM customer\u0026quot;)\n\ncountDF: org.apache.spark.sql.DataFrame \u003d [cnt: bigint]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that the sql method returns a DataFrame. Let’s discuss how to view or retrieve the contents of a DataFrame in a moment.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003etoDF\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe toDF method allows you to rename the columns in the source DataFrame. It takes the new names of the columns as arguments and returns a new DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval resultDF \u003d sqlContext.sql(\u0026quot;SELECT count(1) from customer\u0026quot;)\n\nresultDF: org.apache.spark.sql.DataFrame \u003d [_c0: bigint]\n\nval countDF \u003d resultDF.toDF(\u0026quot;cnt\u0026quot;)\n\ncountDF: org.apache.spark.sql.DataFrame \u003d [cnt: bigint]\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488326391902_1468710071",
      "id": "20170301-005951_335034470",
      "dateCreated": "Mar 1, 2017 12:59:51 AM",
      "dateStarted": "Mar 1, 2017 1:25:11 AM",
      "dateFinished": "Mar 1, 2017 1:25:11 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nLanguage-Integrated Query Methods\n---\n\n**agg**\n\nThe agg method performs specified aggregations on one or more columns in the source DataFrame and returns the result as a new DataFrame.\n~~~\nval aggregates \u003d productDF.agg(max(\"price\"), min(\"price\"), count(\"name\"))\n\naggregates: org.apache.spark.sql.DataFrame \u003d [max(price): double, min(price): double, count(name): bigint]\n\naggregates.show\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     200.0|          6|\n+----------+----------+-----------+\n~~~\nLet’s discuss the show method in a little bit. Essentially, it displays the content of a DataFrame on the console.\n\n**apply**\n\nThe apply method takes the name of a column as an argument and returns the specified column in the source DataFrame as an instance of the Column class. The Column class provides operators for manipulating a column in a DataFrame.\n~~~\nval priceColumn \u003d productDF.apply(\"price\")\n\npriceColumn: org.apache.spark.sql.Column \u003d price\n\nval discountedPriceColumn \u003d priceColumn * 0.5\n\ndiscountedPriceColumn: org.apache.spark.sql.Column \u003d (price * 0.5)\n~~~~\n\nScala provides syntactic sugar that allows you to use productDF(\"price\") instead of productDF.apply(\"price\"). It automatically converts productDF(\"price\") to productDF.apply(\"price\"). So the preceding code can be rewritten, as shown next.\n\n~~~\nval priceColumn \u003d productDF(\"price\")\nval discountedPriceColumn \u003d priceColumn * 0.5\n~~~~\n\nAn instance of the Column class is generally used as an input to some of the DataFrame methods or functions defined in the Spark SQL library. Let’s revisit one of the examples discussed earlier.\n~~~\nval aggregates \u003d productDF.agg(max(\"price\"), min(\"price\"), count(\"name\"))\n~~~\nIt is a concise version of the statement shown next.\n~~~\nval aggregates \u003d productDF.agg(max(productDF(\"price\")), min(productDF(\"price\")),\n                               count(productDF(\"name\")))\n~~~\nThe expression productDF(\"price\") can also be written as $\"price\" for convenience. Thus, the following two expressions are equivalent.\n~~~\nval aggregates \u003d productDF.agg(max($\"price\"), min($\"price\"), count($\"name\"))\nval aggregates \u003d productDF.agg(max(productDF(\"price\")), min(productDF(\"price\")),\n                               count(productDF(\"name\")))\n~~~\nIf a method or function expects an instance of the Column class as an argument, you can use the $\"...\" notation to select a column in a DataFrame.\n\nIn summary, the following three statements are equivalent.\n~~~\nval aggregates \u003d productDF.agg(max(productDF(\"price\")), min(productDF(\"price\")),\n                               count(productDF(\"name\")))\nval aggregates \u003d productDF.agg(max(\"price\"), min(\"price\"), count(\"name\"))\nval aggregates \u003d productDF.agg(max($\"price\"), min($\"price\"), count($\"name\"))\n~~~\n**cube**\n\nThe cube method takes the names of one or more columns as arguments and returns a cube for multi-dimensional analysis. It is useful for generating cross-tabular reports.\n\nAssume you have a dataset that tracks sales along three dimensions: time, product and country. The cube method allows you to generate aggregates for all the possible combinations of the dimensions that you are interested in.\n~~~\ncase class SalesSummary(date: String, product: String, country: String, revenue: Double)\nval sales \u003d List(SalesSummary(\"01/01/2015\", \"iPhone\", \"USA\", 40000),\n                 SalesSummary(\"01/02/2015\", \"iPhone\", \"USA\", 30000),\n                 SalesSummary(\"01/01/2015\", \"iPhone\", \"China\", 10000),\n                 SalesSummary(\"01/02/2015\", \"iPhone\", \"China\", 5000),\n                 SalesSummary(\"01/01/2015\", \"S6\", \"USA\", 20000),\n                 SalesSummary(\"01/02/2015\", \"S6\", \"USA\", 10000),\n                 SalesSummary(\"01/01/2015\", \"S6\", \"China\", 9000),\n                 SalesSummary(\"01/02/2015\", \"S6\", \"China\", 6000))\n\nval salesDF \u003d sc.parallelize(sales).toDF()\n\nval salesCubeDF \u003d salesDF.cube($\"date\", $\"product\", $\"country\").sum(\"revenue\")\n\nsalesCubeDF: org.apache.spark.sql.DataFrame \u003d [date: string, product: string, country: string, sum(revenue): double]\n\nsalesCubeDF.withColumnRenamed(\"sum(revenue)\", \"total\").show(30)\n~~~\n+----------+-------+-------+--------+\n|      date|product|country|   total|\n+----------+-------+-------+--------+\n|01/01/2015|   null|    USA| 60000.0|\n|01/02/2015|     S6|   null| 16000.0|\n|01/01/2015| iPhone|   null| 50000.0|\n|01/01/2015|     S6|  China|  9000.0|\n|      null|   null|  China| 30000.0|\n|01/02/2015|     S6|    USA| 10000.0|\n|01/02/2015|   null|   null| 51000.0|\n|01/02/2015| iPhone|  China|  5000.0|\n|01/01/2015| iPhone|    USA| 40000.0|\n|01/01/2015|   null|  China| 19000.0|\n|01/02/2015|   null|    USA| 40000.0|\n|      null| iPhone|  China| 15000.0|\n|01/02/2015|     S6|  China|  6000.0|\n|01/01/2015| iPhone|  China| 10000.0|\n|01/02/2015|   null|  China| 11000.0|\n|      null| iPhone|   null| 85000.0|\n|      null| iPhone|    USA| 70000.0|\n|      null|     S6|   null| 45000.0|\n|      null|     S6|    USA| 30000.0|\n|01/01/2015|     S6|   null| 29000.0|\n|      null|   null|   null|130000.0|\n|01/02/2015| iPhone|   null| 35000.0|\n|01/01/2015|     S6|    USA| 20000.0|\n|      null|   null|    USA|100000.0|\n|01/01/2015|   null|   null| 79000.0|\n|      null|     S6|  China| 15000.0|\n|01/02/2015| iPhone|    USA| 30000.0|\n+----------+-------+-------+--------+\n\nIf you wanted to find the total sales of all products in the USA, you can use the following expression.\n~~~\nsalesCubeDF.filter(\"product IS null AND date IS null AND country\u003d\u0027USA\u0027\").show\n~~~\n+----+-------+-------+------------+\n|date|product|country|sum(revenue)|\n+----+-------+-------+------------+\n|null|   null|    USA|    100000.0|\n+----+-------+-------+------------+\n\nIf you wanted to know the subtotal of sales by product in the USA, you can use the following expression.\n~~~\nsalesCubeDF.filter(\"date IS null AND product IS NOT null AND country\u003d\u0027USA\u0027\").show\n~~~\n+----+-------+-------+------------+\n|date|product|country|sum(revenue)|\n+----+-------+-------+------------+\n|null| iPhone|    USA|     70000.0|\n|null|     S6|    USA|     30000.0|\n+----+-------+-------+------------+\n\n**distinct**\n\nThe distinct method returns a new DataFrame containing only the unique rows in the source DataFrame.\n~~~\nval dfWithoutDuplicates \u003d customerDF.distinct\n~~~\n**explode**\n\nThe explode method generates zero or more rows from a column using a user-provided function. It takes three arguments. The first argument is the input column, the second argument is the output column and the third argument is a user provided function that generates one or more values for the output column for each value in the input column.\n\nFor example, consider a dataset that has a text column containing contents of an email. Let’s assume that you want to split the email content into individual words and you want a row for each word in an email.\n~~~\ncase class Email(sender: String, recepient: String, subject: String, body: String)\nval emails \u003d List(Email(\"James\", \"Mary\", \"back\", \"just got back from vacation\"),\n                  Email(\"John\", \"Jessica\", \"money\", \"make million dollars\"),\n                  Email(\"Tim\", \"Kevin\", \"report\", \"send me sales report ASAP\"))\n\nval emailDF \u003d sc.parallelize(emails).toDF()\nval wordDF \u003d emailDF.explode(\"body\", \"word\") { body: String \u003d\u003e body.split(\" \")}\nwordDF.show\n\n+------+---------+-------+--------------------+--------+\n|sender|recepient|subject|                body|    word|\n+------+---------+-------+--------------------+--------+\n| James|     Mary|   back|just got back fro...|    just|\n| James|     Mary|   back|just got back fro...|     got|\n| James|     Mary|   back|just got back fro...|    back|\n| James|     Mary|   back|just got back fro...|    from|\n| James|     Mary|   back|just got back fro...|vacation|\n|  John|  Jessica|  money|make million dollars|    make|\n|  John|  Jessica|  money|make million dollars| million|\n|  John|  Jessica|  money|make million dollars| dollars|\n|   Tim|    Kevin| report|send me sales rep...|    send|\n|   Tim|    Kevin| report|send me sales rep...|      me|\n|   Tim|    Kevin| report|send me sales rep...|   sales|\n|   Tim|    Kevin| report|send me sales rep...|  report|\n|   Tim|    Kevin| report|send me sales rep...|    ASAP|\n+------+---------+-------+--------------------+--------+\n~~~\n**filter**\n\nThe filter method filters rows in the source DataFrame using a SQL expression provided to it as an argument. It returns a new DataFrame containing only the filtered rows. The SQL expression can be passed as a string argument.\n~~~\nval filteredDF \u003d customerDF.filter(\"age \u003e 25\")\n\nfilteredDF: org.apache.spark.sql.DataFrame \u003d [cId: bigint, name: string, age: int, gender: string]\n\nfilteredDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n~~~\n\nA variant of the filter method allows a filter condition to be specified using the Column type.\n~~~\nval filteredDF \u003d customerDF.filter($\"age\" \u003e 25)\n~~~\nAs mentioned earlier, the preceding code is a short-hand for the following code.\n~~~\nval filteredDF \u003d customerDF.filter(customerDF(\"age\") \u003e 25)\n~~~\n**groupBy**\n\nThe groupBy method groups the rows in the source DataFrame using the columns provided to it as arguments. Aggregation can be performed on the grouped data returned by this method.\n~~~\nval countByGender \u003d customerDF.groupBy(\"gender\").count\n\ncountByGender: org.apache.spark.sql.DataFrame \u003d [gender: string, count: bigint]\n\ncountByGender.show\n\n+------+-----+\n|gender|count|\n+------+-----+\n|     F|    3|\n|     M|    3|\n+------+-----+\n~~~\n~~~\nval revenueByProductDF \u003d salesDF.groupBy(\"product\").sum(\"revenue\")\n\nrevenueByProductDF: org.apache.spark.sql.DataFrame \u003d [product: string, sum(revenue): double]\n\nrevenueByProductDF.show\n\n+-------+------------+\n|product|sum(revenue)|\n+-------+------------+\n| iPhone|     85000.0|\n|     S6|     45000.0|\n+-------+------------+\n~~~\n**intersect**\n\nThe intersect method takes a DataFrame as an argument and returns a new DataFrame containing only the rows in both the input and source DataFrame.\n~~~\nval customers2 \u003d List(Customer(11, \"Jackson\", 21, \"M\"),\n                     Customer(12, \"Emma\", 25, \"F\"),\n                     Customer(13, \"Olivia\", 31, \"F\"),\n                     Customer(4, \"Jennifer\", 45, \"F\"),\n                     Customer(5, \"Robert\", 41, \"M\"),\n                     Customer(6, \"Sandra\", 45, \"F\"))\n\nval customer2DF \u003d sc.parallelize(customers2).toDF()\nval commonCustomersDF \u003d customerDF.intersect(customer2DF)\ncommonCustomersDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  6|  Sandra| 45|     F|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n+---+--------+---+------+\n~~~\n**join**\n\nThe join method performs a SQL join of the source DataFrame with another DataFrame. It takes three arguments, a DataFrame, a join expression and a join type.\n~~~\ncase class Transaction(tId: Long, custId: Long, prodId: Long, date: String, city: String)\nval transactions \u003d List(Transaction(1, 5, 3, \"01/01/2015\", \"San Francisco\"),\n                        Transaction(2, 6, 1, \"01/02/2015\", \"San Jose\"),\n                        Transaction(3, 1, 6, \"01/01/2015\", \"Boston\"),\n                        Transaction(4, 200, 400, \"01/02/2015\", \"Palo Alto\"),\n                        Transaction(6, 100, 100, \"01/02/2015\", \"Mountain View\"))\n\nval transactionDF \u003d sc.parallelize(transactions).toDF()\nval innerDF \u003d transactionDF.join(customerDF, $\"custId\" \u003d\u003d\u003d $\"cId\", \"inner\")\n\ninnerDF.show\n\n+---+------+------+----------+-------------+---+------+---+------+\n|tId|custId|prodId|      date|         city|cId|  name|age|gender|\n+---+------+------+----------+-------------+---+------+---+------+\n|  1|     5|     3|01/01/2015|San Francisco|  5|Robert| 41|     M|\n|  2|     6|     1|01/02/2015|     San Jose|  6|Sandra| 45|     F|\n|  3|     1|     6|01/01/2015|       Boston|  1| James| 21|     M|\n+---+------+------+----------+-------------+---+------+---+------+\n\nval outerDF \u003d transactionDF.join(customerDF, $\"custId\" \u003d\u003d\u003d $\"cId\", \"outer\")\nouterDF.show\n\n+----+------+------+----------+-------------+----+--------+----+------+\n| tId|custId|prodId|      date|         city| cId|    name| age|gender|\n+----+------+------+----------+-------------+----+--------+----+------+\n|   6|   100|   100|01/02/2015|Mountain View|null|    null|null|  null|\n|   4|   200|   400|01/02/2015|    Palo Alto|null|    null|null|  null|\n|   3|     1|     6|01/01/2015|       Boston|   1|   James|  21|     M|\n|null|  null|  null|      null|         null|   2|     Liz|  25|     F|\n|null|  null|  null|      null|         null|   3|    John|  31|     M|\n|null|  null|  null|      null|         null|   4|Jennifer|  45|     F|\n|   1|     5|     3|01/01/2015|San Francisco|   5|  Robert|  41|     M|\n|   2|     6|     1|01/02/2015|     San Jose|   6|  Sandra|  45|     F|\n+----+------+------+----------+-------------+----+--------+----+------+\n\nval leftOuterDF \u003d transactionDF.join(customerDF, $\"custId\" \u003d\u003d\u003d $\"cId\", \"left_outer\")\nleftOuterDF.show\n\n+---+------+------+----------+-------------+----+------+----+------+\n|tId|custId|prodId|      date|         city| cId|  name| age|gender|\n+---+------+------+----------+-------------+----+------+----+------+\n|  1|     5|     3|01/01/2015|San Francisco|   5|Robert|  41|     M|\n|  2|     6|     1|01/02/2015|     San Jose|   6|Sandra|  45|     F|\n|  3|     1|     6|01/01/2015|       Boston|   1| James|  21|     M|\n|  4|   200|   400|01/02/2015|    Palo Alto|null|  null|null|  null|\n|  6|   100|   100|01/02/2015|Mountain View|null|  null|null|  null|\n+---+------+------+----------+-------------+----+------+----+------+\n\nval rightOuterDF \u003d transactionDF.join(customerDF, $\"custId\" \u003d\u003d\u003d $\"cId\", \"right_outer\")\nrightOuterDF.show\n\n+----+------+------+----------+-------------+---+--------+---+------+\n| tId|custId|prodId|      date|         city|cId|    name|age|gender|\n+----+------+------+----------+-------------+---+--------+---+------+\n|   3|     1|     6|01/01/2015|       Boston|  1|   James| 21|     M|\n|null|  null|  null|      null|         null|  2|     Liz| 25|     F|\n|null|  null|  null|      null|         null|  3|    John| 31|     M|\n|null|  null|  null|      null|         null|  4|Jennifer| 45|     F|\n|   1|     5|     3|01/01/2015|San Francisco|  5|  Robert| 41|     M|\n|   2|     6|     1|01/02/2015|     San Jose|  6|  Sandra| 45|     F|\n+----+------+------+----------+-------------+---+--------+---+------+\n~~~\n**limit**\n\nThe limit method returns a DataFrame containing the specified number of rows from the source DataFrame.\n~~~\nval fiveCustomerDF \u003d customerDF.limit(5)\nfiveCustomer.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  2|     Liz| 25|     F|\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n+---+--------+---+------+\n~~~\n**orderBy**\n\nThe orderBy method returns a DataFrame sorted by the given columns. It takes the names of one or more columns as arguments.\n~~~\nval sortedDF \u003d customerDF.orderBy(\"name\")\nsortedDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  4|Jennifer| 45|     F|\n|  3|    John| 31|     M|\n|  2|     Liz| 25|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n~~~\nBy default, the orderBy method sorts in ascending order. You can explicitly specify the sorting order using a Column expression, as shown next.\n~~~\nval sortedByAgeNameDF \u003d customerDF.sort($\"age\".desc, $\"name\".asc)\nsortedByAgeNameDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  4|Jennifer| 45|     F|\n|  6|  Sandra| 45|     F|\n|  5|  Robert| 41|     M|\n|  3|    John| 31|     M|\n|  2|     Liz| 25|     F|\n|  1|   James| 21|     M|\n+---+--------+---+------+\n~~~\n**randomSplit**\n\nThe randomSplit method splits the source DataFrame into multiple DataFrames. It takes an array of weights as argument and returns an array of DataFrames. It is a useful method for machine learning, where you want to split the raw dataset into training, validation and test datasets.\n~~~\nval dfArray \u003d homeDF.randomSplit(Array(0.6, 0.2, 0.2))\ndfArray(0).count\ndfArray(1).count\ndfArray(2).count\n~~~\n**rollup**\n\nThe rollup method takes the names of one or more columns as arguments and returns a multi-dimensional rollup. It is useful for subaggregation along a hierarchical dimension such as geography or time.\n\nAssume you have a dataset that tracks annual sales by city, state and country. The rollup method can be used to calculate both grand total and subtotals by city, state, and country.\n~~~\ncase class SalesByCity(year: Int, city: String, state: String,\n                       country: String, revenue: Double)\nval salesByCity \u003d List(SalesByCity(2014, \"Boston\", \"MA\", \"USA\", 2000),\n                       SalesByCity(2015, \"Boston\", \"MA\", \"USA\", 3000),\n                       SalesByCity(2014, \"Cambridge\", \"MA\", \"USA\", 2000),\n                       SalesByCity(2015, \"Cambridge\", \"MA\", \"USA\", 3000),\n                       SalesByCity(2014, \"Palo Alto\", \"CA\", \"USA\", 4000),\n                       SalesByCity(2015, \"Palo Alto\", \"CA\", \"USA\", 6000),\n                       SalesByCity(2014, \"Pune\", \"MH\", \"India\", 1000),\n                       SalesByCity(2015, \"Pune\", \"MH\", \"India\", 1000),\n                       SalesByCity(2015, \"Mumbai\", \"MH\", \"India\", 1000),\n                       SalesByCity(2014, \"Mumbai\", \"MH\", \"India\", 2000))\n\nval salesByCityDF \u003d sc.parallelize(salesByCity).toDF()\nval rollup \u003d salesByCityDF.rollup($\"country\", $\"state\", $\"city\").sum(\"revenue\")\nrollup.show\n\n+-------+-----+---------+------------+\n|country|state|     city|sum(revenue)|\n+-------+-----+---------+------------+\n|  India|   MH|   Mumbai|      3000.0|\n|    USA|   MA|Cambridge|      5000.0|\n|  India|   MH|     Pune|      2000.0|\n|    USA|   MA|   Boston|      5000.0|\n|    USA|   MA|     null|     10000.0|\n|    USA| null|     null|     20000.0|\n|    USA|   CA|     null|     10000.0|\n|   null| null|     null|     25000.0|\n|  India|   MH|     null|      5000.0|\n|    USA|   CA|Palo Alto|     10000.0|\n|  India| null|     null|      5000.0|\n+-------+-----+---------+------------+\n~~~\n**sample**\n\nThe sample method returns a DataFrame containing the specified fraction of the rows in the source DataFrame. It takes two arguments. The first argument is a Boolean value indicating whether sampling should be done with replacement. The second argument specifies the fraction of the rows that should be returned.\n~~~\nval sampleDF \u003d homeDF.sample(true, 0.10)\n~~~\n**select**\n\nThe select method returns a DataFrame containing only the specified columns from the source DataFrame.\n~~~\nval namesAgeDF \u003d customerDF.select(\"name\", \"age\")\nnamesAgeDF.show\n\n+--------+---+\n|    name|age|\n+--------+---+\n|   James| 21|\n|     Liz| 25|\n|    John| 31|\n|Jennifer| 45|\n|  Robert| 41|\n|  Sandra| 45|\n+--------+---+\n~~~\nA variant of the select method allows one or more Column expressions as arguments.\n~~~\nval newAgeDF \u003d customerDF.select($\"name\", $\"age\" + 10)\nnewAgeDF.show\n\n+--------+----------+\n|    name|(age + 10)|\n+--------+----------+\n|   James|        31|\n|     Liz|        35|\n|    John|        41|\n|Jennifer|        55|\n|  Robert|        51|\n|  Sandra|        55|\n+--------+----------+\n~~~\n**selectExpr**\n\nThe selectExpr method accepts one or more SQL expressions as arguments and returns a DataFrame generated by executing the specified SQL expressions.\n~~~\nval newCustomerDF \u003d customerDF.selectExpr(\"name\", \"age + 10  AS new_age\",\n                                          \"IF(gender \u003d \u0027M\u0027, true, false) AS male\")\n\nnewCustomerDF.show\n\n+--------+-------+-----+\n|    name|new_age| male|\n+--------+-------+-----+\n|   James|     31| true|\n|     Liz|     35|false|\n|    John|     41| true|\n|Jennifer|     55|false|\n|  Robert|     51| true|\n|  Sandra|     55|false|\n+--------+-------+-----+\n~~~\n**withColumn**\n\nThe withColumn method adds a new column to or replaces an existing column in the source DataFrame and returns a new DataFrame. It takes two arguments. The first argument is the name of the new column and the second argument is an expression for generating the values of the new column.\n~~~\nval newProductDF \u003d productDF.withColumn(\"profit\", $\"price\" - $\"cost\")\nnewProductDF.show\n\n+---+-------+------+-----+------+\n|pId|   name| price| cost|profit|\n+---+-------+------+-----+------+\n|  1| iPhone| 600.0|400.0| 200.0|\n|  2| Galaxy| 500.0|400.0| 100.0|\n|  3|   iPad| 400.0|300.0| 100.0|\n|  4| Kindle| 200.0|100.0| 100.0|\n|  5|MacBook|1200.0|900.0| 300.0|\n|  6|   Dell| 500.0|400.0| 100.0|\n+---+-------+------+-----+------+\n~~~\n\n\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 1:44:20 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eLanguage-Integrated Query Methods\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eagg\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe agg method performs specified aggregations on one or more columns in the source DataFrame and returns the result as a new DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval aggregates \u003d productDF.agg(max(\u0026quot;price\u0026quot;), min(\u0026quot;price\u0026quot;), count(\u0026quot;name\u0026quot;))\n\naggregates: org.apache.spark.sql.DataFrame \u003d [max(price): double, min(price): double, count(name): bigint]\n\naggregates.show\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     200.0|          6|\n+----------+----------+-----------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet’s discuss the show method in a little bit. Essentially, it displays the content of a DataFrame on the console.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eapply\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe apply method takes the name of a column as an argument and returns the specified column in the source DataFrame as an instance of the Column class. The Column class provides operators for manipulating a column in a DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval priceColumn \u003d productDF.apply(\u0026quot;price\u0026quot;)\n\npriceColumn: org.apache.spark.sql.Column \u003d price\n\nval discountedPriceColumn \u003d priceColumn * 0.5\n\ndiscountedPriceColumn: org.apache.spark.sql.Column \u003d (price * 0.5)\n~~~~\n\nScala provides syntactic sugar that allows you to use productDF(\u0026quot;price\u0026quot;) instead of productDF.apply(\u0026quot;price\u0026quot;). It automatically converts productDF(\u0026quot;price\u0026quot;) to productDF.apply(\u0026quot;price\u0026quot;). So the preceding code can be rewritten, as shown next.\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eval priceColumn \u003d productDF(\u0026ldquo;price\u0026rdquo;)\u003cbr/\u003eval discountedPriceColumn \u003d priceColumn * 0.5\u003cbr/\u003e~~~~\u003c/p\u003e\n\u003cp\u003eAn instance of the Column class is generally used as an input to some of the DataFrame methods or functions defined in the Spark SQL library. Let’s revisit one of the examples discussed earlier.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval aggregates \u003d productDF.agg(max(\u0026quot;price\u0026quot;), min(\u0026quot;price\u0026quot;), count(\u0026quot;name\u0026quot;))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt is a concise version of the statement shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval aggregates \u003d productDF.agg(max(productDF(\u0026quot;price\u0026quot;)), min(productDF(\u0026quot;price\u0026quot;)),\n                               count(productDF(\u0026quot;name\u0026quot;)))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe expression productDF(\u0026ldquo;price\u0026rdquo;) can also be written as $\u0026ldquo;price\u0026rdquo; for convenience. Thus, the following two expressions are equivalent.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval aggregates \u003d productDF.agg(max($\u0026quot;price\u0026quot;), min($\u0026quot;price\u0026quot;), count($\u0026quot;name\u0026quot;))\nval aggregates \u003d productDF.agg(max(productDF(\u0026quot;price\u0026quot;)), min(productDF(\u0026quot;price\u0026quot;)),\n                               count(productDF(\u0026quot;name\u0026quot;)))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf a method or function expects an instance of the Column class as an argument, you can use the $\u0026ldquo;\u0026hellip;\u0026rdquo; notation to select a column in a DataFrame.\u003c/p\u003e\n\u003cp\u003eIn summary, the following three statements are equivalent.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval aggregates \u003d productDF.agg(max(productDF(\u0026quot;price\u0026quot;)), min(productDF(\u0026quot;price\u0026quot;)),\n                               count(productDF(\u0026quot;name\u0026quot;)))\nval aggregates \u003d productDF.agg(max(\u0026quot;price\u0026quot;), min(\u0026quot;price\u0026quot;), count(\u0026quot;name\u0026quot;))\nval aggregates \u003d productDF.agg(max($\u0026quot;price\u0026quot;), min($\u0026quot;price\u0026quot;), count($\u0026quot;name\u0026quot;))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ecube\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe cube method takes the names of one or more columns as arguments and returns a cube for multi-dimensional analysis. It is useful for generating cross-tabular reports.\u003c/p\u003e\n\u003cp\u003eAssume you have a dataset that tracks sales along three dimensions: time, product and country. The cube method allows you to generate aggregates for all the possible combinations of the dimensions that you are interested in.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class SalesSummary(date: String, product: String, country: String, revenue: Double)\nval sales \u003d List(SalesSummary(\u0026quot;01/01/2015\u0026quot;, \u0026quot;iPhone\u0026quot;, \u0026quot;USA\u0026quot;, 40000),\n                 SalesSummary(\u0026quot;01/02/2015\u0026quot;, \u0026quot;iPhone\u0026quot;, \u0026quot;USA\u0026quot;, 30000),\n                 SalesSummary(\u0026quot;01/01/2015\u0026quot;, \u0026quot;iPhone\u0026quot;, \u0026quot;China\u0026quot;, 10000),\n                 SalesSummary(\u0026quot;01/02/2015\u0026quot;, \u0026quot;iPhone\u0026quot;, \u0026quot;China\u0026quot;, 5000),\n                 SalesSummary(\u0026quot;01/01/2015\u0026quot;, \u0026quot;S6\u0026quot;, \u0026quot;USA\u0026quot;, 20000),\n                 SalesSummary(\u0026quot;01/02/2015\u0026quot;, \u0026quot;S6\u0026quot;, \u0026quot;USA\u0026quot;, 10000),\n                 SalesSummary(\u0026quot;01/01/2015\u0026quot;, \u0026quot;S6\u0026quot;, \u0026quot;China\u0026quot;, 9000),\n                 SalesSummary(\u0026quot;01/02/2015\u0026quot;, \u0026quot;S6\u0026quot;, \u0026quot;China\u0026quot;, 6000))\n\nval salesDF \u003d sc.parallelize(sales).toDF()\n\nval salesCubeDF \u003d salesDF.cube($\u0026quot;date\u0026quot;, $\u0026quot;product\u0026quot;, $\u0026quot;country\u0026quot;).sum(\u0026quot;revenue\u0026quot;)\n\nsalesCubeDF: org.apache.spark.sql.DataFrame \u003d [date: string, product: string, country: string, sum(revenue): double]\n\nsalesCubeDF.withColumnRenamed(\u0026quot;sum(revenue)\u0026quot;, \u0026quot;total\u0026quot;).show(30)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e+\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026ndash;+\u003cbr/\u003e| date|product|country| total|\u003cbr/\u003e+\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026ndash;+\u003cbr/\u003e|01/01/2015| null| USA| 60000.0|\u003cbr/\u003e|01/02/2015| S6| null| 16000.0|\u003cbr/\u003e|01/01/2015| iPhone| null| 50000.0|\u003cbr/\u003e|01/01/2015| S6| China| 9000.0|\u003cbr/\u003e| null| null| China| 30000.0|\u003cbr/\u003e|01/02/2015| S6| USA| 10000.0|\u003cbr/\u003e|01/02/2015| null| null| 51000.0|\u003cbr/\u003e|01/02/2015| iPhone| China| 5000.0|\u003cbr/\u003e|01/01/2015| iPhone| USA| 40000.0|\u003cbr/\u003e|01/01/2015| null| China| 19000.0|\u003cbr/\u003e|01/02/2015| null| USA| 40000.0|\u003cbr/\u003e| null| iPhone| China| 15000.0|\u003cbr/\u003e|01/02/2015| S6| China| 6000.0|\u003cbr/\u003e|01/01/2015| iPhone| China| 10000.0|\u003cbr/\u003e|01/02/2015| null| China| 11000.0|\u003cbr/\u003e| null| iPhone| null| 85000.0|\u003cbr/\u003e| null| iPhone| USA| 70000.0|\u003cbr/\u003e| null| S6| null| 45000.0|\u003cbr/\u003e| null| S6| USA| 30000.0|\u003cbr/\u003e|01/01/2015| S6| null| 29000.0|\u003cbr/\u003e| null| null| null|130000.0|\u003cbr/\u003e|01/02/2015| iPhone| null| 35000.0|\u003cbr/\u003e|01/01/2015| S6| USA| 20000.0|\u003cbr/\u003e| null| null| USA|100000.0|\u003cbr/\u003e|01/01/2015| null| null| 79000.0|\u003cbr/\u003e| null| S6| China| 15000.0|\u003cbr/\u003e|01/02/2015| iPhone| USA| 30000.0|\u003cbr/\u003e+\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026ndash;+\u003c/p\u003e\n\u003cp\u003eIf you wanted to find the total sales of all products in the USA, you can use the following expression.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esalesCubeDF.filter(\u0026quot;product IS null AND date IS null AND country\u003d\u0026#39;USA\u0026#39;\u0026quot;).show\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003cbr/\u003e|date|product|country|sum(revenue)|\u003cbr/\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003cbr/\u003e|null| null| USA| 100000.0|\u003cbr/\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003c/p\u003e\n\u003cp\u003eIf you wanted to know the subtotal of sales by product in the USA, you can use the following expression.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esalesCubeDF.filter(\u0026quot;date IS null AND product IS NOT null AND country\u003d\u0026#39;USA\u0026#39;\u0026quot;).show\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003cbr/\u003e|date|product|country|sum(revenue)|\u003cbr/\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003cbr/\u003e|null| iPhone| USA| 70000.0|\u003cbr/\u003e|null| S6| USA| 30000.0|\u003cbr/\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003edistinct\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe distinct method returns a new DataFrame containing only the unique rows in the source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval dfWithoutDuplicates \u003d customerDF.distinct\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eexplode\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe explode method generates zero or more rows from a column using a user-provided function. It takes three arguments. The first argument is the input column, the second argument is the output column and the third argument is a user provided function that generates one or more values for the output column for each value in the input column.\u003c/p\u003e\n\u003cp\u003eFor example, consider a dataset that has a text column containing contents of an email. Let’s assume that you want to split the email content into individual words and you want a row for each word in an email.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class Email(sender: String, recepient: String, subject: String, body: String)\nval emails \u003d List(Email(\u0026quot;James\u0026quot;, \u0026quot;Mary\u0026quot;, \u0026quot;back\u0026quot;, \u0026quot;just got back from vacation\u0026quot;),\n                  Email(\u0026quot;John\u0026quot;, \u0026quot;Jessica\u0026quot;, \u0026quot;money\u0026quot;, \u0026quot;make million dollars\u0026quot;),\n                  Email(\u0026quot;Tim\u0026quot;, \u0026quot;Kevin\u0026quot;, \u0026quot;report\u0026quot;, \u0026quot;send me sales report ASAP\u0026quot;))\n\nval emailDF \u003d sc.parallelize(emails).toDF()\nval wordDF \u003d emailDF.explode(\u0026quot;body\u0026quot;, \u0026quot;word\u0026quot;) { body: String \u003d\u0026gt; body.split(\u0026quot; \u0026quot;)}\nwordDF.show\n\n+------+---------+-------+--------------------+--------+\n|sender|recepient|subject|                body|    word|\n+------+---------+-------+--------------------+--------+\n| James|     Mary|   back|just got back fro...|    just|\n| James|     Mary|   back|just got back fro...|     got|\n| James|     Mary|   back|just got back fro...|    back|\n| James|     Mary|   back|just got back fro...|    from|\n| James|     Mary|   back|just got back fro...|vacation|\n|  John|  Jessica|  money|make million dollars|    make|\n|  John|  Jessica|  money|make million dollars| million|\n|  John|  Jessica|  money|make million dollars| dollars|\n|   Tim|    Kevin| report|send me sales rep...|    send|\n|   Tim|    Kevin| report|send me sales rep...|      me|\n|   Tim|    Kevin| report|send me sales rep...|   sales|\n|   Tim|    Kevin| report|send me sales rep...|  report|\n|   Tim|    Kevin| report|send me sales rep...|    ASAP|\n+------+---------+-------+--------------------+--------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003efilter\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe filter method filters rows in the source DataFrame using a SQL expression provided to it as an argument. It returns a new DataFrame containing only the filtered rows. The SQL expression can be passed as a string argument.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval filteredDF \u003d customerDF.filter(\u0026quot;age \u0026gt; 25\u0026quot;)\n\nfilteredDF: org.apache.spark.sql.DataFrame \u003d [cId: bigint, name: string, age: int, gender: string]\n\nfilteredDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA variant of the filter method allows a filter condition to be specified using the Column type.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval filteredDF \u003d customerDF.filter($\u0026quot;age\u0026quot; \u0026gt; 25)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs mentioned earlier, the preceding code is a short-hand for the following code.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval filteredDF \u003d customerDF.filter(customerDF(\u0026quot;age\u0026quot;) \u0026gt; 25)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003egroupBy\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe groupBy method groups the rows in the source DataFrame using the columns provided to it as arguments. Aggregation can be performed on the grouped data returned by this method.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval countByGender \u003d customerDF.groupBy(\u0026quot;gender\u0026quot;).count\n\ncountByGender: org.apache.spark.sql.DataFrame \u003d [gender: string, count: bigint]\n\ncountByGender.show\n\n+------+-----+\n|gender|count|\n+------+-----+\n|     F|    3|\n|     M|    3|\n+------+-----+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eval revenueByProductDF \u003d salesDF.groupBy(\u0026quot;product\u0026quot;).sum(\u0026quot;revenue\u0026quot;)\n\nrevenueByProductDF: org.apache.spark.sql.DataFrame \u003d [product: string, sum(revenue): double]\n\nrevenueByProductDF.show\n\n+-------+------------+\n|product|sum(revenue)|\n+-------+------------+\n| iPhone|     85000.0|\n|     S6|     45000.0|\n+-------+------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eintersect\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe intersect method takes a DataFrame as an argument and returns a new DataFrame containing only the rows in both the input and source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval customers2 \u003d List(Customer(11, \u0026quot;Jackson\u0026quot;, 21, \u0026quot;M\u0026quot;),\n                     Customer(12, \u0026quot;Emma\u0026quot;, 25, \u0026quot;F\u0026quot;),\n                     Customer(13, \u0026quot;Olivia\u0026quot;, 31, \u0026quot;F\u0026quot;),\n                     Customer(4, \u0026quot;Jennifer\u0026quot;, 45, \u0026quot;F\u0026quot;),\n                     Customer(5, \u0026quot;Robert\u0026quot;, 41, \u0026quot;M\u0026quot;),\n                     Customer(6, \u0026quot;Sandra\u0026quot;, 45, \u0026quot;F\u0026quot;))\n\nval customer2DF \u003d sc.parallelize(customers2).toDF()\nval commonCustomersDF \u003d customerDF.intersect(customer2DF)\ncommonCustomersDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  6|  Sandra| 45|     F|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ejoin\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe join method performs a SQL join of the source DataFrame with another DataFrame. It takes three arguments, a DataFrame, a join expression and a join type.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class Transaction(tId: Long, custId: Long, prodId: Long, date: String, city: String)\nval transactions \u003d List(Transaction(1, 5, 3, \u0026quot;01/01/2015\u0026quot;, \u0026quot;San Francisco\u0026quot;),\n                        Transaction(2, 6, 1, \u0026quot;01/02/2015\u0026quot;, \u0026quot;San Jose\u0026quot;),\n                        Transaction(3, 1, 6, \u0026quot;01/01/2015\u0026quot;, \u0026quot;Boston\u0026quot;),\n                        Transaction(4, 200, 400, \u0026quot;01/02/2015\u0026quot;, \u0026quot;Palo Alto\u0026quot;),\n                        Transaction(6, 100, 100, \u0026quot;01/02/2015\u0026quot;, \u0026quot;Mountain View\u0026quot;))\n\nval transactionDF \u003d sc.parallelize(transactions).toDF()\nval innerDF \u003d transactionDF.join(customerDF, $\u0026quot;custId\u0026quot; \u003d\u003d\u003d $\u0026quot;cId\u0026quot;, \u0026quot;inner\u0026quot;)\n\ninnerDF.show\n\n+---+------+------+----------+-------------+---+------+---+------+\n|tId|custId|prodId|      date|         city|cId|  name|age|gender|\n+---+------+------+----------+-------------+---+------+---+------+\n|  1|     5|     3|01/01/2015|San Francisco|  5|Robert| 41|     M|\n|  2|     6|     1|01/02/2015|     San Jose|  6|Sandra| 45|     F|\n|  3|     1|     6|01/01/2015|       Boston|  1| James| 21|     M|\n+---+------+------+----------+-------------+---+------+---+------+\n\nval outerDF \u003d transactionDF.join(customerDF, $\u0026quot;custId\u0026quot; \u003d\u003d\u003d $\u0026quot;cId\u0026quot;, \u0026quot;outer\u0026quot;)\nouterDF.show\n\n+----+------+------+----------+-------------+----+--------+----+------+\n| tId|custId|prodId|      date|         city| cId|    name| age|gender|\n+----+------+------+----------+-------------+----+--------+----+------+\n|   6|   100|   100|01/02/2015|Mountain View|null|    null|null|  null|\n|   4|   200|   400|01/02/2015|    Palo Alto|null|    null|null|  null|\n|   3|     1|     6|01/01/2015|       Boston|   1|   James|  21|     M|\n|null|  null|  null|      null|         null|   2|     Liz|  25|     F|\n|null|  null|  null|      null|         null|   3|    John|  31|     M|\n|null|  null|  null|      null|         null|   4|Jennifer|  45|     F|\n|   1|     5|     3|01/01/2015|San Francisco|   5|  Robert|  41|     M|\n|   2|     6|     1|01/02/2015|     San Jose|   6|  Sandra|  45|     F|\n+----+------+------+----------+-------------+----+--------+----+------+\n\nval leftOuterDF \u003d transactionDF.join(customerDF, $\u0026quot;custId\u0026quot; \u003d\u003d\u003d $\u0026quot;cId\u0026quot;, \u0026quot;left_outer\u0026quot;)\nleftOuterDF.show\n\n+---+------+------+----------+-------------+----+------+----+------+\n|tId|custId|prodId|      date|         city| cId|  name| age|gender|\n+---+------+------+----------+-------------+----+------+----+------+\n|  1|     5|     3|01/01/2015|San Francisco|   5|Robert|  41|     M|\n|  2|     6|     1|01/02/2015|     San Jose|   6|Sandra|  45|     F|\n|  3|     1|     6|01/01/2015|       Boston|   1| James|  21|     M|\n|  4|   200|   400|01/02/2015|    Palo Alto|null|  null|null|  null|\n|  6|   100|   100|01/02/2015|Mountain View|null|  null|null|  null|\n+---+------+------+----------+-------------+----+------+----+------+\n\nval rightOuterDF \u003d transactionDF.join(customerDF, $\u0026quot;custId\u0026quot; \u003d\u003d\u003d $\u0026quot;cId\u0026quot;, \u0026quot;right_outer\u0026quot;)\nrightOuterDF.show\n\n+----+------+------+----------+-------------+---+--------+---+------+\n| tId|custId|prodId|      date|         city|cId|    name|age|gender|\n+----+------+------+----------+-------------+---+--------+---+------+\n|   3|     1|     6|01/01/2015|       Boston|  1|   James| 21|     M|\n|null|  null|  null|      null|         null|  2|     Liz| 25|     F|\n|null|  null|  null|      null|         null|  3|    John| 31|     M|\n|null|  null|  null|      null|         null|  4|Jennifer| 45|     F|\n|   1|     5|     3|01/01/2015|San Francisco|  5|  Robert| 41|     M|\n|   2|     6|     1|01/02/2015|     San Jose|  6|  Sandra| 45|     F|\n+----+------+------+----------+-------------+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003elimit\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe limit method returns a DataFrame containing the specified number of rows from the source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval fiveCustomerDF \u003d customerDF.limit(5)\nfiveCustomer.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  2|     Liz| 25|     F|\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eorderBy\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe orderBy method returns a DataFrame sorted by the given columns. It takes the names of one or more columns as arguments.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval sortedDF \u003d customerDF.orderBy(\u0026quot;name\u0026quot;)\nsortedDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  4|Jennifer| 45|     F|\n|  3|    John| 31|     M|\n|  2|     Liz| 25|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBy default, the orderBy method sorts in ascending order. You can explicitly specify the sorting order using a Column expression, as shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval sortedByAgeNameDF \u003d customerDF.sort($\u0026quot;age\u0026quot;.desc, $\u0026quot;name\u0026quot;.asc)\nsortedByAgeNameDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  4|Jennifer| 45|     F|\n|  6|  Sandra| 45|     F|\n|  5|  Robert| 41|     M|\n|  3|    John| 31|     M|\n|  2|     Liz| 25|     F|\n|  1|   James| 21|     M|\n+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003erandomSplit\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe randomSplit method splits the source DataFrame into multiple DataFrames. It takes an array of weights as argument and returns an array of DataFrames. It is a useful method for machine learning, where you want to split the raw dataset into training, validation and test datasets.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval dfArray \u003d homeDF.randomSplit(Array(0.6, 0.2, 0.2))\ndfArray(0).count\ndfArray(1).count\ndfArray(2).count\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003erollup\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe rollup method takes the names of one or more columns as arguments and returns a multi-dimensional rollup. It is useful for subaggregation along a hierarchical dimension such as geography or time.\u003c/p\u003e\n\u003cp\u003eAssume you have a dataset that tracks annual sales by city, state and country. The rollup method can be used to calculate both grand total and subtotals by city, state, and country.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class SalesByCity(year: Int, city: String, state: String,\n                       country: String, revenue: Double)\nval salesByCity \u003d List(SalesByCity(2014, \u0026quot;Boston\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;USA\u0026quot;, 2000),\n                       SalesByCity(2015, \u0026quot;Boston\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;USA\u0026quot;, 3000),\n                       SalesByCity(2014, \u0026quot;Cambridge\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;USA\u0026quot;, 2000),\n                       SalesByCity(2015, \u0026quot;Cambridge\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;USA\u0026quot;, 3000),\n                       SalesByCity(2014, \u0026quot;Palo Alto\u0026quot;, \u0026quot;CA\u0026quot;, \u0026quot;USA\u0026quot;, 4000),\n                       SalesByCity(2015, \u0026quot;Palo Alto\u0026quot;, \u0026quot;CA\u0026quot;, \u0026quot;USA\u0026quot;, 6000),\n                       SalesByCity(2014, \u0026quot;Pune\u0026quot;, \u0026quot;MH\u0026quot;, \u0026quot;India\u0026quot;, 1000),\n                       SalesByCity(2015, \u0026quot;Pune\u0026quot;, \u0026quot;MH\u0026quot;, \u0026quot;India\u0026quot;, 1000),\n                       SalesByCity(2015, \u0026quot;Mumbai\u0026quot;, \u0026quot;MH\u0026quot;, \u0026quot;India\u0026quot;, 1000),\n                       SalesByCity(2014, \u0026quot;Mumbai\u0026quot;, \u0026quot;MH\u0026quot;, \u0026quot;India\u0026quot;, 2000))\n\nval salesByCityDF \u003d sc.parallelize(salesByCity).toDF()\nval rollup \u003d salesByCityDF.rollup($\u0026quot;country\u0026quot;, $\u0026quot;state\u0026quot;, $\u0026quot;city\u0026quot;).sum(\u0026quot;revenue\u0026quot;)\nrollup.show\n\n+-------+-----+---------+------------+\n|country|state|     city|sum(revenue)|\n+-------+-----+---------+------------+\n|  India|   MH|   Mumbai|      3000.0|\n|    USA|   MA|Cambridge|      5000.0|\n|  India|   MH|     Pune|      2000.0|\n|    USA|   MA|   Boston|      5000.0|\n|    USA|   MA|     null|     10000.0|\n|    USA| null|     null|     20000.0|\n|    USA|   CA|     null|     10000.0|\n|   null| null|     null|     25000.0|\n|  India|   MH|     null|      5000.0|\n|    USA|   CA|Palo Alto|     10000.0|\n|  India| null|     null|      5000.0|\n+-------+-----+---------+------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003esample\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe sample method returns a DataFrame containing the specified fraction of the rows in the source DataFrame. It takes two arguments. The first argument is a Boolean value indicating whether sampling should be done with replacement. The second argument specifies the fraction of the rows that should be returned.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval sampleDF \u003d homeDF.sample(true, 0.10)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eselect\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe select method returns a DataFrame containing only the specified columns from the source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval namesAgeDF \u003d customerDF.select(\u0026quot;name\u0026quot;, \u0026quot;age\u0026quot;)\nnamesAgeDF.show\n\n+--------+---+\n|    name|age|\n+--------+---+\n|   James| 21|\n|     Liz| 25|\n|    John| 31|\n|Jennifer| 45|\n|  Robert| 41|\n|  Sandra| 45|\n+--------+---+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA variant of the select method allows one or more Column expressions as arguments.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval newAgeDF \u003d customerDF.select($\u0026quot;name\u0026quot;, $\u0026quot;age\u0026quot; + 10)\nnewAgeDF.show\n\n+--------+----------+\n|    name|(age + 10)|\n+--------+----------+\n|   James|        31|\n|     Liz|        35|\n|    John|        41|\n|Jennifer|        55|\n|  Robert|        51|\n|  Sandra|        55|\n+--------+----------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eselectExpr\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe selectExpr method accepts one or more SQL expressions as arguments and returns a DataFrame generated by executing the specified SQL expressions.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval newCustomerDF \u003d customerDF.selectExpr(\u0026quot;name\u0026quot;, \u0026quot;age + 10  AS new_age\u0026quot;,\n                                          \u0026quot;IF(gender \u003d \u0026#39;M\u0026#39;, true, false) AS male\u0026quot;)\n\nnewCustomerDF.show\n\n+--------+-------+-----+\n|    name|new_age| male|\n+--------+-------+-----+\n|   James|     31| true|\n|     Liz|     35|false|\n|    John|     41| true|\n|Jennifer|     55|false|\n|  Robert|     51| true|\n|  Sandra|     55|false|\n+--------+-------+-----+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ewithColumn\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe withColumn method adds a new column to or replaces an existing column in the source DataFrame and returns a new DataFrame. It takes two arguments. The first argument is the name of the new column and the second argument is an expression for generating the values of the new column.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval newProductDF \u003d productDF.withColumn(\u0026quot;profit\u0026quot;, $\u0026quot;price\u0026quot; - $\u0026quot;cost\u0026quot;)\nnewProductDF.show\n\n+---+-------+------+-----+------+\n|pId|   name| price| cost|profit|\n+---+-------+------+-----+------+\n|  1| iPhone| 600.0|400.0| 200.0|\n|  2| Galaxy| 500.0|400.0| 100.0|\n|  3|   iPad| 400.0|300.0| 100.0|\n|  4| Kindle| 200.0|100.0| 100.0|\n|  5|MacBook|1200.0|900.0| 300.0|\n|  6|   Dell| 500.0|400.0| 100.0|\n+---+-------+------+-----+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488326340874_-654996589",
      "id": "20170301-005900_363428902",
      "dateCreated": "Mar 1, 2017 12:59:00 AM",
      "dateStarted": "Mar 1, 2017 1:44:21 AM",
      "dateFinished": "Mar 1, 2017 1:44:21 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nRDD Operations\n---\n\nThe DataFrame class supports commonly used RDD operations such as map, flatMap, foreach, foreachPartition, mapPartition, coalesce, and repartition. These methods work similar to their namesake operations in the RDD class.\n\nIn addition, if you need access to other RDD methods that are not present in the DataFrame class, you can get an RDD from a DataFrame. This section discusses the commonly used techniques for generating an RDD from a DataFrame.\n\n**rdd**\n\nrdd is defined as a lazy val in the DataFrame class. It represents the source DataFrame as an RDD of Row instances.\n\nAs discussed earlier, a Row represents a relational tuple in the source DataFrame. It allows both generic access and native primitive access of fields by their ordinal.\n\nAn example is shown next.\n~~~\nval rdd \u003d customerDF.rdd\n\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[405] at rdd at \u003cconsole\u003e:27\n\nval firstRow \u003d rdd.first\n\nfirstRow: org.apache.spark.sql.Row \u003d [1,James,21,M]\n\nval name \u003d firstRow.getString(1)\n\nname: String \u003d James\n\nval age \u003d firstRow.getInt(2)\n\nage: Int \u003d 21\n~~~\nFields in a Row can also be extracted using Scala pattern matching.\n~~~\nimport org.apache.spark.sql.Row\nval rdd \u003d customerDF.rdd\n\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[113] at rdd at \u003cconsole\u003e:28\n\nval nameAndAge \u003d rdd.map {\n                   case Row(cId: Long, name: String, age: Int, gender: String) \u003d\u003e (name, age)\n                 }\n\nnameAndAge: org.apache.spark.rdd.RDD[(String, Int)] \u003d MapPartitionsRDD[114] at map at \u003cconsole\u003e:30\n\nnameAndAge.collect\n\nres79: Array[(String, Int)] \u003d Array((James,21), (Liz,25), (John,31), (Jennifer,45), (Robert,41), (Sandra,45))\n~~~\n**toJSON**\n\nThe toJSON method generates an RDD of JSON strings from the source DataFrame. Each element in the returned RDD is a JSON object.\n~~~\nval jsonRDD \u003d customerDF.toJSON\n\njsonRDD: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[408] at toJSON at \u003cconsole\u003e:28\n\njsonRDD.collect\n\nres80: Array[String] \u003d Array({\"cId\":1,\"name\":\"James\",\"age\":21,\"gender\":\"M\"}, {\"cId\":2,\"name\":\"Liz\",\"age\":25,\"gender\":\"F\"}, {\"cId\":3,\"name\":\"John\",\"age\":31,\"gender\":\"M\"}, {\"cId\":4,\"name\":\"Jennifer\",\"age\":45,\"gender\":\"F\"}, {\"cId\":5,\"name\":\"Robert\",\"age\":41,\"gender\":\"M\"}, {\"cId\":6,\"name\":\"Sandra\",\"age\":45,\"gender\":\"F\"})\n~~~\n**Actions**\n\nSimilar to the RDD actions, the action methods in the DataFrame class return results to the Driver program. This section covers the commonly used action methods in the DataFrame class.\n\n**collect**\n\nThe collect method returns the data in a DataFrame as an array of Rows.\n~~~\nval result \u003d customerDF.collect\n\nresult: Array[org.apache.spark.sql.Row] \u003d Array([1,James,21,M], [2,Liz,25,F], [3,John,31,M], [4,Jennifer,45,F], [5,Robert,41,M], [6,Sandra,45,F])\n~~~\n**count**\n\nThe count method returns the number of rows in the source DataFrame.\n~~~\nval count \u003d customerDF.count\n\ncount: Long \u003d 6\n~~~\n**describe**\n\nThe describe method can be used for exploratory data analysis. It returns summary statistics for numeric columns in the source DataFrame. The summary statistics includes min, max, count, mean, and standard deviation. It takes the names of one or more columns as arguments.\n~~~\nval summaryStatsDF \u003d productDF.describe(\"price\", \"cost\")\n\nsummaryStatsDF: org.apache.spark.sql.DataFrame \u003d [summary: string, price: string, cost: string]\n\nsummaryStatsDF.show\n\n+-------+------------------+------------------+\n|summary|             price|              cost|\n+-------+------------------+------------------+\n|  count|                 6|                 6|\n|   mean| 566.6666666666666| 416.6666666666667|\n| stddev|309.12061651652357|240.94720491334928|\n|    min|             200.0|             100.0|\n|    max|            1200.0|             900.0|\n+-------+------------------+------------------+\n~~~\n**first**\n\nThe first method returns the first row in the source DataFrame.\n~~~\nval first \u003d customerDF.first\n\nfirst: org.apache.spark.sql.Row \u003d [1,James,21,M]\n~~~\n**show**\n\nThe show method displays the rows in the source DataFrame on the driver console in a tabular format. It optionally takes an integer N as an argument and displays the top N rows. If no argument is provided, it shows the top 20 rows.\n~~~\ncustomerDF.show(2)\n\n+---+-----+---+------+\n|cId| name|age|gender|\n+---+-----+---+------+\n|  1|James| 21|     M|\n|  2|  Liz| 25|     F|\n+---+-----+---+------+\nonly showing top 2 rows\n~~~\n**take**\n\nThe take method takes an integer N as an argument and returns the first N rows from the source DataFrame as an array of Rows.\n~~~\nval first2Rows \u003d customerDF.take(2)\n\nfirst2Rows: Array[org.apache.spark.sql.Row] \u003d Array([1,James,21,M], [2,Liz,25,F])\n~~~\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 2:05:48 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eRDD Operations\u003c/h2\u003e\n\u003cp\u003eThe DataFrame class supports commonly used RDD operations such as map, flatMap, foreach, foreachPartition, mapPartition, coalesce, and repartition. These methods work similar to their namesake operations in the RDD class.\u003c/p\u003e\n\u003cp\u003eIn addition, if you need access to other RDD methods that are not present in the DataFrame class, you can get an RDD from a DataFrame. This section discusses the commonly used techniques for generating an RDD from a DataFrame.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003erdd\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003erdd is defined as a lazy val in the DataFrame class. It represents the source DataFrame as an RDD of Row instances.\u003c/p\u003e\n\u003cp\u003eAs discussed earlier, a Row represents a relational tuple in the source DataFrame. It allows both generic access and native primitive access of fields by their ordinal.\u003c/p\u003e\n\u003cp\u003eAn example is shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval rdd \u003d customerDF.rdd\n\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[405] at rdd at \u0026lt;console\u0026gt;:27\n\nval firstRow \u003d rdd.first\n\nfirstRow: org.apache.spark.sql.Row \u003d [1,James,21,M]\n\nval name \u003d firstRow.getString(1)\n\nname: String \u003d James\n\nval age \u003d firstRow.getInt(2)\n\nage: Int \u003d 21\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFields in a Row can also be extracted using Scala pattern matching.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.sql.Row\nval rdd \u003d customerDF.rdd\n\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[113] at rdd at \u0026lt;console\u0026gt;:28\n\nval nameAndAge \u003d rdd.map {\n                   case Row(cId: Long, name: String, age: Int, gender: String) \u003d\u0026gt; (name, age)\n                 }\n\nnameAndAge: org.apache.spark.rdd.RDD[(String, Int)] \u003d MapPartitionsRDD[114] at map at \u0026lt;console\u0026gt;:30\n\nnameAndAge.collect\n\nres79: Array[(String, Int)] \u003d Array((James,21), (Liz,25), (John,31), (Jennifer,45), (Robert,41), (Sandra,45))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003etoJSON\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe toJSON method generates an RDD of JSON strings from the source DataFrame. Each element in the returned RDD is a JSON object.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval jsonRDD \u003d customerDF.toJSON\n\njsonRDD: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[408] at toJSON at \u0026lt;console\u0026gt;:28\n\njsonRDD.collect\n\nres80: Array[String] \u003d Array({\u0026quot;cId\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;James\u0026quot;,\u0026quot;age\u0026quot;:21,\u0026quot;gender\u0026quot;:\u0026quot;M\u0026quot;}, {\u0026quot;cId\u0026quot;:2,\u0026quot;name\u0026quot;:\u0026quot;Liz\u0026quot;,\u0026quot;age\u0026quot;:25,\u0026quot;gender\u0026quot;:\u0026quot;F\u0026quot;}, {\u0026quot;cId\u0026quot;:3,\u0026quot;name\u0026quot;:\u0026quot;John\u0026quot;,\u0026quot;age\u0026quot;:31,\u0026quot;gender\u0026quot;:\u0026quot;M\u0026quot;}, {\u0026quot;cId\u0026quot;:4,\u0026quot;name\u0026quot;:\u0026quot;Jennifer\u0026quot;,\u0026quot;age\u0026quot;:45,\u0026quot;gender\u0026quot;:\u0026quot;F\u0026quot;}, {\u0026quot;cId\u0026quot;:5,\u0026quot;name\u0026quot;:\u0026quot;Robert\u0026quot;,\u0026quot;age\u0026quot;:41,\u0026quot;gender\u0026quot;:\u0026quot;M\u0026quot;}, {\u0026quot;cId\u0026quot;:6,\u0026quot;name\u0026quot;:\u0026quot;Sandra\u0026quot;,\u0026quot;age\u0026quot;:45,\u0026quot;gender\u0026quot;:\u0026quot;F\u0026quot;})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eActions\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSimilar to the RDD actions, the action methods in the DataFrame class return results to the Driver program. This section covers the commonly used action methods in the DataFrame class.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ecollect\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe collect method returns the data in a DataFrame as an array of Rows.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval result \u003d customerDF.collect\n\nresult: Array[org.apache.spark.sql.Row] \u003d Array([1,James,21,M], [2,Liz,25,F], [3,John,31,M], [4,Jennifer,45,F], [5,Robert,41,M], [6,Sandra,45,F])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ecount\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe count method returns the number of rows in the source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval count \u003d customerDF.count\n\ncount: Long \u003d 6\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003edescribe\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe describe method can be used for exploratory data analysis. It returns summary statistics for numeric columns in the source DataFrame. The summary statistics includes min, max, count, mean, and standard deviation. It takes the names of one or more columns as arguments.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval summaryStatsDF \u003d productDF.describe(\u0026quot;price\u0026quot;, \u0026quot;cost\u0026quot;)\n\nsummaryStatsDF: org.apache.spark.sql.DataFrame \u003d [summary: string, price: string, cost: string]\n\nsummaryStatsDF.show\n\n+-------+------------------+------------------+\n|summary|             price|              cost|\n+-------+------------------+------------------+\n|  count|                 6|                 6|\n|   mean| 566.6666666666666| 416.6666666666667|\n| stddev|309.12061651652357|240.94720491334928|\n|    min|             200.0|             100.0|\n|    max|            1200.0|             900.0|\n+-------+------------------+------------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003efirst\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe first method returns the first row in the source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval first \u003d customerDF.first\n\nfirst: org.apache.spark.sql.Row \u003d [1,James,21,M]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eshow\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe show method displays the rows in the source DataFrame on the driver console in a tabular format. It optionally takes an integer N as an argument and displays the top N rows. If no argument is provided, it shows the top 20 rows.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.show(2)\n\n+---+-----+---+------+\n|cId| name|age|gender|\n+---+-----+---+------+\n|  1|James| 21|     M|\n|  2|  Liz| 25|     F|\n+---+-----+---+------+\nonly showing top 2 rows\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003etake\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe take method takes an integer N as an argument and returns the first N rows from the source DataFrame as an array of Rows.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval first2Rows \u003d customerDF.take(2)\n\nfirst2Rows: Array[org.apache.spark.sql.Row] \u003d Array([1,James,21,M], [2,Liz,25,F])\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488329060646_-1912250369",
      "id": "20170301-014420_1842446467",
      "dateCreated": "Mar 1, 2017 1:44:20 AM",
      "dateStarted": "Mar 1, 2017 2:05:44 AM",
      "dateFinished": "Mar 1, 2017 2:05:44 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 2:01:40 AM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1488330100561_723753525",
      "id": "20170301-020140_719756754",
      "dateCreated": "Mar 1, 2017 2:01:40 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/CursoAmadeus2017/SparkSQL",
  "id": "2CCTD7RNN",
  "angularObjects": {
    "2CC52E4S5:shared_process": [],
    "2C9N1T5Y3:shared_process": [],
    "2CAZ8JZD9:shared_process": [],
    "2CCZDGC3H:shared_process": [],
    "2C9S66NQK:shared_process": [],
    "2C94SRCMC:shared_process": [],
    "2CCCN7GZD:shared_process": [],
    "2CCGRTB7B:shared_process": [],
    "2CC8YMXCQ:shared_process": [],
    "2CC6XAQM3:shared_process": [],
    "2C9731R8U:shared_process": [],
    "2CBZVGS5S:shared_process": [],
    "2CBDXBFEU:shared_process": [],
    "2CB3WM78Y:shared_process": [],
    "2CCN2DPHQ:shared_process": [],
    "2C9GE2AUU:shared_process": [],
    "2CCV4F9FS:shared_process": [],
    "2CCHNGUC9:shared_process": [],
    "2C9RQJYPD:shared_process": []
  },
  "config": {
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}